{"./":{"url":"./","title":"开篇","keywords":"","body":"1.1.1. 简介1.1.1. 简介 Flink 官方文档阅读笔记（v1.13.0），所有测试代码会维护在 daily_flink。 学而不思则罔，思而不学则怠。 By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"Overview/":{"url":"Overview/","title":"基础概念","keywords":"","body":" 导航信息 Stateful Stream Processing Timely Stream Processing Flink Architecture Flink API By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"Overview/stateful_stream_processing.html":{"url":"Overview/stateful_stream_processing.html","title":"Stateful Stream Processing","keywords":"","body":"1.1.1. 什么是状态？1.1.2. Keyed State1.1.3. 状态持久化1.1.4. Checkpointing1.1.5. Unaligned Checkpointing1.1.6. State Backends1.1.7. Savepoints1.1.8. Exactly Once vs. At Least Once1.1.9. State and Fault Tolerance in Batch Programs1.1.10. 思考1.1.1. 什么是状态？ 在一条数据流中，很多操作一次只处理一个事件，比如事件解析器，就是简单的 ETL 操作。但是某些操作需要记住已经处理过的事件的信息，比如流式处理中每小时的请求量，也就是窗口操作，我们管这些操作叫作有状态的，已经发生处理过的事件会影响后续的统计。 下面是一些有状态的操作的例子： 当应用程序需要按照某些模式搜索事件时，需要记录下来已经发生过的事件。 当按照日期聚合事件时，需要记录下来该日期内发生的事件当做状态。 当训练机器学习模型时，需要用状态记录下来当前的模型版本，参数等信息。 当需要处理历史事件时，需要用状态记录过去发生的事件，这样才有权限访问统计。 Flink 使用 Checkpoints 和 Savepoints 来做容错管理和精确处理一次的处理语义，因此需要管理状态，记录各个操作的状态。 需要注意的是 Flink 重启并且并行度改变时，状态也需要分发至不同节点，并重新分配。这点在 failover 时会讲到。 Queryable state允许你在 flink 程序运行时管理状态，即 state processer API，这些在任务状态初始化，状态错误修改场景下比较有用。 当处理状态时，先了解 Flink’s state backends非常有用，这会告诉我们 Flink 支持哪几种状态后端，状态以何种方式存在哪里。 1.1.2. Keyed State Keyed State 可以被认为是以键值对方式存储的状态。举个例子，统计当天每个商品的售出，在某一时刻，商品 A 售出了 10 件，那么 A->10 就是此刻的状态，当下次再接收到 A 卖出一件的事件时，状态就变成了 A->11，checkpoint 中存储的状态就是截止某一时间点前某个 key 的当前状态。 Keyed State 与读取该 key 的流一起被分发，因此，只能在 keyd stream 上操作键值对状态。理解一下，也就是 keyed state 与下游该 key 的 operator 是绑定在一起的，这个 operator 可以更新插入新的 keyed state，并且在状态和 operator 对齐情况下都是本地操作，没有事务开销，保证一致性。这种对齐还允许 Flink 重新分发状态和调整流分区。拿官网上这张图举例子来说，Keys(A,B,Y) 被分发到了 task1 上，A,B,Y 的后续状态会在 task1 上进行管理，管理了三个 key 的 state。 Keyed State 被进一步组织成 Keyed Groups，这是 Flink 重新分发 Keyed State 的最小单位，Keyed Groups 的数量与设置的最大并行度一致。 在执行过程中，Keyed Operator 的每个并行实例都与一个或多个 Key Groups 的键一起工作。 1.1.3. 状态持久化 Flink 使用流重放和 Checkpoints 来实现容错。每个 checkpoint 都是输入流中一个特定点以及这个特定点时间当时各个 operator 的状态。一个实时数据流可以从上次成功的 checkpoint 恢复，恢复各个 operator 的状态，并重放数据，重新处理上次 checkpoint 后来的数据。checkpoint 的间隔需要衡量容错开销与恢复时间（需要重新处理的记录数）。 这种容错机制会源源不断的产生分布式快照，对于状态比较小的流式处理程序，即使 checkpoint 比较频繁，这种方式也比较轻量且对性能没有太大影响。checkpoint 被存储在配置的位置，往往是一个分布式文件系统中。 在程序由于机器，网络，处理逻辑问题挂掉后，flink 会将程序重置到上次成功的 checkpoint 处，重启后的 flink 程序并不会影响以前产生的 checkpoint。 checkpoint 默认是关闭的，可以阅读 Checkpointing 查看如何配置开启。 为了保证 checkpoint 机制正常使用，数据源（例如消息队列）需要支持数据重放。Kafka 具有这种能力，并且对应的 Flink connector 也支持。可以去 Fault Tolerance Guarantees of Data Sources and Sinks 查看更多信息。 因为 Flink 的 checkpoint 使用分布式快照实现，所以我们经常用快照表示 checkpoint 或者 savepoint。 1.1.4. Checkpointing Flink 容错机制的核心就是产生数据流和对应 operator 状态的分布式一致性快照。这些快照充当一致性检查点，以便在任务失败时可以回退重启。需要注意的是，checkpoint 是异步完成的。状态的分布式一致性快照。这些快照充当一致性检查点，以便在任务失败时可以回退重启。需要注意的是，checkpoint barriers 不会对操作加锁，因此 operator 可以异步持久化他们的状态。 从 Flink 1.11 开始，checkpoints 可以选择对齐或者不对齐，下面我们首先介绍对齐情况下的 checkpoints。 Barriers Flink 分布式快照中的核心是 stream barriers，暂且叫它流式屏障吧。这些 barriers 被注入到数据流中随着数据一起流动，barriers 永远不会超过最新记录，并且是严格顺序的。一个 barrier 将记录分成进入当前快照和进入下一个快照的记录。每个 barrier 携带者快照 id。barriers 不会中断数据流的处理，因此非常轻量。在同一时间数据流中可能存在多个 barrier，这意味着不同的快照在同时并行发生。 stream barriers 从读取数据源开始，就被注入到数据流中，随着数据源一起流动，在某一时刻，barrier 流动到点 n，这意味着快照 n 将会覆盖数据源 n 位置之前的所有数据。比如，在 Kafka 中，这个位置是分区 offset。这个位置 n 会被提交给 JM 上的 coordinator 进程，用于后续协调整个作业的 checkpoint 生成。 barrier 接着会流往下游，当下游中间的算子接受到所有代表 n 位置的 barrier 时，它继续将 barrier n 发往下游，如此往复，直到 sink operator 从其上游所有输入流中接收到 barrier n 。此时会通知 JM 上的 checkpoint 管理进程该 sink operator 的快照 n 已经准备好了，当所有的 sink operator 都接收到快照 n 时，checkpoint n 的产生也随之完成。一旦快照 n 完成，Flink 应用程序将不会再访问 barrier n 之前的数据。 对于有多个输入流的 operator，需要对齐 barrier。上图说明了这一点 下游 operator 接受到一条上游数据输入流发过来的 barrier n ，直到它的所有输入流都接受到 barrier n，这个 operator 才可以处理位置 n 之后的数据。否则的话，快照 n 里将混合位置 n 和 n+1 之后的数据。（这样来想，如果 barrier n 不等待对齐，而是继续发往下游，处理新的数据，最终快的 barrier 先到达 sink 处，并继续处理数据，等较慢的 barrier 到达 sink 时，请求 JM 生成快照 n，此时生成的快照 n 还是只处理的位置 n 之前的数据吗？已经不是了，快的 task 已经处理了位置 n 之后的数据，这样的检查点在任务重启时，肯定会重复处理数据，也就无法做到精确一次处理。） 一旦某个并发的 operator 最后一条流收到了所有的 barrier n，operator 就会继续处理后续的数据，并且发送 barrier n 到下游。 接下来，这个 operator 会继续处理输入流中的数据，在真正读取流数据前，会先从输入缓冲区中读取数据。 最后，operator 会将自己的状态异步写入状态后端中。 需要注意的是对齐操作适用于多条输入流的 operator，比如多分区的 kafka topic 数据源、发生 shuffle 过程后，需要读取上游数据的下游 operator。 Snapshotting Operator State 当 operator 本身也包含状态时，这种状态也必须是快照的一部分。 Operators 接收到来自所有输入流的 barrier 后，在发送 barrier 到下游数据前快照自己的状态。在此时，barrier 之前的数据都已经被处理，状态也已经记录。因为快照一般比较大，所以会存储到配置的状态后端（比如 HDFS），默认情况下是使用 JM 的内存。当状态被存储下来后，operator 会通知检查点，并且会向下游输出流发送 barrier。 生成的快照现在包含 对于每一个并行的数据源，快照对应的起始读取位置。 对于每一个 operator，存储指向对应状态的指针。此处猜想一下，checkpoint 存储应该分为 metadata 和 operator state 两大部分，在存储 operator state 的过程中，将元数据提交给 JM coordinator 进程，缓存元数据，最后在 sink operator 完成后，元数据完全写入状态后端，最终形成一个完整的 checkpoint。 Recovery 这种机制下的任务恢复比较简单，当任务失败重启时，Flink 选择最新成功的 checkpoint 并应用。程序会重新提交 job grpgh，并将检查点的 operator state 应用到对应的 operator 上，source operator 也会被重置到上次 checkpoint 记录的位置，对于 kafka 来说，就是重置 offset。 如果快照是增量更新的话，operator 会从最新的完整状态的快照开始，并逐步应用最新的增量快照。 可以查看 Restart Strategies获取更多信息。 1.1.5. Unaligned Checkpointing 非对齐的 checkpoint。 checkpoint 允许不对齐 barrier，其基本核心思想是当某个 operator 的某条输入流的 barrier 到来时，不在等待未到达 barrier 的输入流，而是立即像下游发送 barrier，并通知 JM 生成 operator 的局部快照，这样做会造成 checkpoint 不对齐，因而需要在这个快照中保存较慢输入流未处理的数据，从而在 failover 时能重放状态，也就是让程序知道每个 operator 的每条输入流处理到了什么位置，咋听起来，这种方式更复杂。 官网的图感觉不太形象，没太理解。在这里，个人理解下上面的流程 operator 首先接收到上游的第一个 barrier，及 3 之前的数据，并放在了 input buffer 中。 接下来把 3 这个 barrier 立即发往下游，并放在 output buffer 的末端。 operator 会被超越的过的数据，及相对 3 这个 barrire 来说，其他较慢输入流之前的位置的数据进行缓存并生成快照。 因此，非对齐的 checkpoint 仅仅是缓存数据并且转发 barrier，并不需要过多的对齐时间，这点对于吞吐量是有提升的。 需要注意的是 savepoint 往往是对齐的。 Unaligned Recovery 非对齐状态的恢复首先是要恢复 operator 正在处理的数据，其他和对齐 checkpoint 的步骤是一样的。 1.1.6. State Backends 状态存储的数据结构取决于使用哪种状态后端，一种状态后端是内存中的 hashmap，也可以使用 rocksdb 的键值对存储。状态后端可以在不改变应用逻辑的情况下进行配置。 1.1.7. Savepoints checkpoint 是自动的，savepoint 是手动触发的，并且不会自动过期，一般用于手动停止程序后的恢复操作。 1.1.8. Exactly Once vs. At Least Once barrier 的对齐会增加实时流程序处理的延迟，通常，这些延迟在几毫秒内，但是在某些情况下，对齐延迟可能会在分钟甚至小时级别。如果你确实需要超低延时的的流处理，Flink 提供了非对齐的 checkpoint，barrier 会尽可能快的流向 sink，而不需要等待慢的输入流。 在跳过对齐操作后，operator 会持续处理输入，产生的 checkpoint 也是非对齐的，因此在任务 failover，重放数据时，有可能造成数据重复，及至少一次的处理语义。 1.1.9. State and Fault Tolerance in Batch Programs Flink 将批任务运行当做流任务的特例。因此，上述状态的管理程序容错机制同样适用于批处理，不过有以下几点不同： 批处理程序没有检查点，在 failover 时直接重放所有数据，将性能压力放在了 source 端，去除了数据处理时的 checkpoint 消耗。 Dataset 中状态的处理使用内存数据结构，而不是键值对。 1.1.10. 思考 1.Keyed State 中的对齐是指？ 这里没太理解，keyed state 需要对齐的是什么信息呢？ 2.Keyed Groups 是和什么最大并行度保持一致，如何计算出来的呢？ 个人感觉是和 operator 的并行度保持一致（这点需要在后续阅读中验证，也可以查看源码）。在并行度改变时，以 key groups 为单位重新分配 keyed state 到对应的 subtask。 此处可以参考 Flink状态的缩放（rescale）与键组（Key Group）设计 3.在 shuffle 产生数据倾斜时，会不会影响 operator barrier 的对齐过程？ 感觉会影响，下游在读取上游数据时，处理大 key 的 task 肯定比较慢，成为 barrier 对齐等待的一方 4.怎么描述整个 checkpoint 的过程？ 从读取 source 端开始，barrier 被注入到数据流中，下游 operator 在接收到其所有输入流的 barrier 后（即对齐过程），通知 JM checkpoint coordinator 存储状态生成局部快照，记录元数据，并将 barrier 传递给下游，如此往复，直到 sink operator，最终生成全局快照。 5.对齐和不对齐的 checkpoint 的有什么区别，对于性能的影响和考量又在哪里？ 此处可以参考 Flink 1.11 新特性详解:【非对齐】Unaligned Checkpoint 优化高反压 对齐会阻塞数据的数据的处理，在 input buffer 被填满后，还没等到对齐时，会对性能产生影响，增大整个数据处理的延时，降低了程序吞吐量，尤其是在存在反压的时候。但是逻辑清晰，以算子快照为界限分隔。本质上是在最后一个 barrier 到达时触发 checkpoint。 非对齐提高了程序吞吐量，但是由于每个 operator state 缓存当前被 barrier 越过的数据，快照的大小会显著增加，IO 压力会增大。本质上是在第一个 barrier 到达后就触发 checkpoint。 6.checkpoint 相关的配置参数？ 代码 ```java StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // start a checkpoint every 1000 ms env.enableCheckpointing(1000); // advanced options: // set mode to exactly-once (this is the default) env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); // checkpoints have to complete within one minute, or are discarded env.getCheckpointConfig().setCheckpointTimeout(60000); // make sure 500 ms of progress happen between checkpoints env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500); // allow only one checkpoint to be in progress at the same time env.getCheckpointConfig().setMaxConcurrentCheckpoints(1); // enable externalized checkpoints which are retained after job cancellation env.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION); // This determines if a task will be failed if an error occurs in the execution of the task’s checkpoint procedure. env.getCheckpointConfig().setFailOnCheckpointingErrors(true); ``` flink-conf.yml ```yml # 使用何种状态后端 state.backends: filesystme # 检查点的存储目录 state.checkpoint.dir: # 保存点的存储目录 state.savepoints.dir # 是否开启增量快照，默认 false state.backend.incremental: false ``` 7.为什么需要增量快照？ 对于 TB 级别的作业，状态太大，每次做全量快照耗时太长，影响整个链路的处理时延，所以需要增量快照。类似于 Mac 的 TimeMachine，在做完一次全量快照后，剩下的都是增量快照，只处理变化的数据。 那么 Flink 是如何实现增量快照的呢？可以参考 Apache Flink 管理大型状态之增量 Checkpoint 详解 By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"Overview/timely_stream_processing.html":{"url":"Overview/timely_stream_processing.html","title":"Timely Stream Processing","keywords":"","body":"1.1.1. 即席流处理1.1.2. 事件时间 vs 处理时间1.1.3. 事件时间和 watermark1.1.4. 延迟1.1.5. 窗口1.1.6. 思考1.1.1. 即席流处理 这里翻译成了即席流处理，借鉴了即席查询的概念，意味着 Flink 会尽可能快的，以低延迟的方式来处理源源不断的数据。引入时间概念的话，就是 Flink 会按照给定的时间，在触发给定的规则（比如窗口）后尽可能快的处理数据。 emmn，好像跟下面讲的没啥关系。 1.1.2. 事件时间 vs 处理时间 当接触到流数据处理时，其中涉及到的时间概念是不同的（比如说定义窗口的时间）。 Processing time： 处理时间是指正在执行相应操作的机器上的系统时间。当程序使用处理时间时，所有基于时间处理的操作（例如时间窗口）都将基于各个 operator 运行所在的机器的系统时钟。比如，一个小时级别的窗口，程序在 9:15 分开始运行，则该时间窗口内，将会处理 9:15~10:00 间到达 operator 的数据，下一个时间窗口将会处理 10:00~11:00 间的数据。处理时间是最简单的时间概念，不需要数据流与机器之间的协调，它提供了更高的性能和低延时的保证，但是在一个分布式执行环境中，这样带来了幂等性问题，同一批数据经过相同程序计算后的结果是不一样的，并且结果容易受到处理速度及 task 调度的影响。 Event Time：事件时间是事件真实产生的时间，它经常会直接嵌套在记录中进入 Flink 系统，并且每条记录都可以被提取出事件产生的时间戳供 Flink 使用。使用事件时间时，数据流的处理取决于数据本身，与外部时钟无关。在基于事件时间数据流中，程序必须指明如何生成 watermark，从而触发计算。 在理想情况下，基于 Event Time 的流处理不管数据何时到达还是乱序进入，都能保证一致确定的结果。然后，在真实世界中，往往存在乱序数据，在等待乱序数据的过程中就会产生延迟，而程序又不能无限等待，由此造成的结果是数据的处理并不是完全正确的。 假设数据都到达了，基于事件时间的操作会如期进行，即使存在迟到数据，重新回溯历史数据的情况，程序也能产生正确幂等的结果。例如，一个基于事件时间的小时窗口中会包含所有 event timestamp 在这个小时内的数据，不管数据乱序还是处理时间超出该小时范围。 需要注意的是，有时候在使用 Event Time 处理实时流时，也会引入 Processing Time 的逻辑用来保证产生的数据可以被及时处理。 1.1.3. 事件时间和 watermark Note：关于 event time 和 watermark 想了解更多的话，可以去查看 DataFlow Model 论文 基于 event time 的事件处理程序需要一种机制来监测事件的进度，从而知道何时该触发计算，何时该丢弃延迟的数据，何时当前时间窗口内不会在接收到新的数据。event time 是独立于处理时间的，例如，一条流中处理时间会大于事件时间，但是他们的速度是相同的，不存在互相等待。另外，一条流也可以在短时间回溯大量历史数据。 Flink 中管理事件进度的机制叫 Watermarks。Watermarks 作为数据流的一部分，携带者事件时间戳 t。Watermark(t) 代表 t 时间前的数据都已经到达，这意味着不会再有 t' 下图说明了一条流中 watermark 的产生，该流的数据是顺序到达的，意味着 watermark 只是简单的定时从事件时间中产生。 Watermarks 在面对无序数据流时表现的很奇怪，时常会让我们感到疑惑，就像下图一样，事件乱序到达。一般来说，watermark 代表小于等于该时间的数据都已经到达，watermark 流到 operator 时，该 operator 就会更新自己的内部时钟到 watermark 处，不再接受其之前的数据。 并行数据流中的 watermark Watermarks 在数据源或之后的一些操作生成。Source 端并行的 subtask 各自生成自己的 watermark，这些 watermark 定义了源数据中的事件时间。随着 watermark 在数据流中的流动，当到达下游 operator 时，该 operator 就会更新自己的内部时钟，并且生成一个新的 watermark 到下游。 对于存在多个输入流的 operator，需要进行 watermark 的对齐，operator 总会使用最小的 watermark 作为更新其内部时钟的基准。下图展示了 watermark 在并行数据流中的流动。 1.1.4. 延迟 Watermarks 的条件并不是完全正确的，在现实世界中，总会存在延迟数据，这意味着总会有 t' Allowed Lateness 了解更多信息。 1.1.5. 窗口 实时数据处理与批处理在聚合操作上的处理是完全不同的，因为实时流是无界的，不可能等待所有的数据都到达后在做聚合，因此 Flink 引入了窗口的概念，比如聚合刚过去的 5min 发生的事件。 窗口可以由时间驱动和事件驱动，常见的窗口类型有 固定窗口（没有重叠）, 滑动窗口（存在重叠）, 会话窗口 (活跃时间间隔划分）. 1.1.6. 思考 1.barriers 和 watermark 都随着数据流移动，这些是怎么实现的呢？ 2.Watermarks 的主要作用？ 触发计算，限制状态的无限增长。事件不可能无限的等待下去，需要在准确性和延时下做好衡量。 3.Watermarks 如何配置？ 这一次带你彻底搞懂 Flink Watermark 4.延迟数据的处理？ 对于无状态的计算，比较容易，可以先旁路输出到延迟队列，在做处理。对于有状态的计算呢？ By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"Overview/flink_architecture.html":{"url":"Overview/flink_architecture.html","title":"Flink Architecture","keywords":"","body":"1.1.1. Flink 基础架构1.1.2. Flink 集群剖析1.1.3. JobManager1.1.4. TaskManagers1.1.5. Tasks and Operator Chains1.1.6. Task Slots and Resources1.1.7. Flink 程序的运行1.1.8. 思考1.1.1. Flink 基础架构 Flink 作为分布式的数据处理引擎，需要高效的资源管理分配方式用以支撑内部流式作业的运行。它集成了常见的资源管理系统，比如 Yarn，Mesos，k8s，也支持以 standalone 的方式运行。本节用来概述 Flink 的架构，介绍 Flink 是如何与客户端应用程序交互的，遇到错误失败时是如何恢复程序的。 1.1.2. Flink 集群剖析 Flink 运行时主要包括两种进程，JobManager 和 TaskManager，TaskManager 可以启动多个。 客户端并不在程序运行时存在，我们只是通过他向 JM 提交一份 job graph。在这之后，如果是 detached mode 的话，客户端会退出，attached mode 的话，客户端会继续和 JM 保持连接获取程序的进度信息。客户端可以作为用户程序的一部分用来触发任务执行，也可以是 ./bin/flink run ... 这个种方式。 JM 和 TM 以不同的方式启动，如果是 standalone 模式，则直接在机器上启动。如果是基于 Yarn，则是直接交由对应的资源管理器进行启动。TM 需要与 JM 保持连接，通知 JM 自己可用，然后由 JM 注册分配 task 到可用的 TM 上。 1.1.3. JobManager JobManager 负责协调管理 Flink 任务的执行，比如：何时开始调度任务执行、对已经完成或者失败的任务作出反应、协调 checkpoint 的生成、管理任务的 failover 等等。它主要由以下三个组件组成 ResourceManager ResourceManager 负责 Flink 集群资源的分配 - 管理 Flink 最小的资源调度单位 task slots。Flink 针对不同的资源管理器提供了对应的 ResourceManager 的实现。在 standalone 模式下，Flink 不能自动创建新的 TaskManager。 Dispatcher Dispatcher 提供了一个 REST 接口用来提交 Flink 作业，每次提交都会生成一个 JobMaster。另外，其还运行着 Flink Web UI。 JobMaster JobMaster 负责管理单个 Job Graph 的运行。 Flink 集群中可以同时运行多个 job，但是每个 job 都有自己单独的 JobMaster。 通常情况下只需要启动一个 JobManager 就 ok，如果要做高可用的话，可以查看 High Availability (HA) 1.1.4. TaskManagers TaskManager 用来实际运行 task，并缓存和交换数据（比如 shuffle 过程中的落盘）。程序运行必须存在一个 TaskManager，TaskManager 中资源调度的最小单位是 slot，它决定了该 TaskManager 上最多可以并行多少 task。需要注意多个 operator 可以运行在一个 subtask 上。 1.1.5. Tasks and Operator Chains Flink 支持链接多个 operator 到一个 subtask（也可以说是一个 slot）中运行。每个 task 单独一个线程执行，多个 operator 链接到一起可以提高程序处理的性能，减少各个线程之间的数据交换及各种 buffer 开销。可以选择是否开启 operator chain。 下图展示了 operator chains，source 和 map 链接到了一起，最终的执行是有 5 个 subtask。 1.1.6. Task Slots and Resources 每个 TaskManager 都是一个 JVM 进程，内部以多线程的方式执行一个或者多个 subtask。该如何控制 TaskManager 上执行的任务数呢？可以通过 task slot 来设置，其最小为 1。 每个 task slot 都代表 TaskManager 资源的一个子集。比如，一个 TM 有三个 slot，则每个 slot 会占用 TM 总资源的 1/3。通过 slot 分配资源意味着不同作业之间 subtask 的运行时互不影响，拥有独立的驻留内存等资源。需要注意这里并不是 CPU 隔离的，slot 只是占有当前 TM 进程的内存资源。 通过指定一定数量的 task slot，用户可以自己决定 subtask 的运行隔离。每个 TM 有一个 slot 代表所有的 subtask 都在不同的 JVM 进程之中运行。每个 TM 拥有多个 slot 代表其中的 subtask 在一个 JVM 进程中。同一进程中的 task 共享 TCP 连接和心跳信息，也可以共享一些数据集，从而减少每个任务的开销。 默认情况下，Flink 允许不同 task 下的 subtask 共享一个 slot，只要他们属于同一个 Job。这样做的结果可能会造成一个 slot 处理了整个 Job 的 task。设计成这样主要有两点好处 Flink 值需要提供跟最大并行度一样的 slot 即可，而不需要计算每个 task 的并行度。 刚好的资源利用率，如果没有共享 slot，资源使用比较少的 source/map task 就会独占整个 slot，直至其运行完成，其中剩余的资源都无法被使用。 1.1.7. Flink 程序的运行 Flink 程序通过 main 方法入口生成一个或者多个作业提交到集群。这些 job 可以在本地 JVM 进程（LocalEnvironment）或者拥有多个机器的远程集群上运行（RemoteEnvironment）。对于每一个程序，ExecutionEnvironment 提供了控制作业执行的方法（比如设置并行度，开启 checkpoint 等）。 Flink 程序可以用 session mode，per job 的方式运行，下面介绍这些运行方式的生命周期及资源管理 Flink Session Cluster 集群生命周期：在 session 模式下，Flink 客户端通过连接一个预先启动，长期运行的集群来提交作业。即使提交的作业已经运行完成，集群（包括 JM）仍然会运行，直到整个 session 被手动停止。Flink Session Cluster 的生命周期与上面运行的作业无关。 资源隔离：在作业提交后，RM 负责分配 task slots，在作业运行完成后就会释放响应资源。因为所有作业共享整个集群，所以存在资源竞争 - 比如提交作业的网络带宽。这种共享的一个限制是一旦 TM 崩溃，在其上面运行的 subtask 都会失败。同样的，一旦 JM 挂掉，整个集群的作业都会失败。 其他注意事项：session 模式下，减小了作业的启停开销，主要是 TM 的启动和资源的申请，这对于启动时间大于运行时间的作业是友好的，场景大概在小任务比较多的情况下使用比较好。 Flink Job Cluster 集群生命周期：在 per-job 模式下，会为每个提交的任务单独启动一个集群供作业单独使用。客户端首先从集群管理器申请资源启动 JM，然后向 Dispatcher 提交作业，然后根据作业的资源信息，启动 TM 进程。一旦该作业完成，整个集群也会被销毁。 资源隔离：JM 产生错误只会影响当前的作业。 其他注意事项：因为 RM 需要申请外部资源并等待管理组件启动 TM，per-job 模式适合运行时间较长，要求高可用性的大型作业。生产环境中通常是这种模式 K8s 不支持 per-job 模式。 Flink Application Cluster 集群生命周期：该模式用来专门运行 Flink 作业，提交作业也在集群上提交而不是客户端。作业提交是一步过程，不需要先启动 JM 在提交。取而代之的是已 jar 包的形式，ApplicationClusterEntryPoint 负责执行 main 方法将作业先转换成 job graph。Flink Application Cluster 的生命周期与 Flink 程序本身相关。 资源隔离：在 Flink Application Cluster 中，RM 和 Dispatcher 作用于单个 Flink 程序，提供了比 session mode 下更好的资源隔离，比 per-job 模式下更少的启动时间。 per-job 模式可以看做运行在客户端上的 Flink Application Cluster 。 1.1.8. 思考 1.从 job 提交来看 Yarn，Flink，Spark？ yarn 只是集群管理器，这里更贴切的是 MR。看看大部分计算框架的作业提交过程，都是相通的，基本思想是一致的。 客户端像外部资源管理器申请资源，启动 master。 master 做好调度，切分任务。 在启动 worker，将切分好的 task 发到各个 worker 上进行。 woker 与 master 之间维持心跳通信，一旦 worker 挂掉或者上面的 task 失败，master 就会接到通知重新申请资源调度起失败任务 最后到任务完成，释放资源。 2.Flink 常见资源配置参数？ 可以查看官方参数配置文档 3.咋一看，Flink Application Cluster 是作业提交运行最佳的方式，是这样吗？ 生产环境上的实时计算平台好像就是这种模式。 By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"Overview/flink_api.html":{"url":"Overview/flink_api.html","title":"Flink API","keywords":"","body":"1.1.1. 基础概念1.1.2. Flink’s APIs1.1.3. 思考1.1.1. 基础概念 Flink 是什么？Flink 是一种支持有状态计算的流式数据处理引擎，并且提供了批数据的处理能力。 1.1.2. Flink’s APIs Flink 针对流/批程序的处理提供了不同层次的 API 抽象。 最底层的 api 抽象是 stateful and timely stream processing，通过 DataStream API 中的的 process function 接口向外暴露。它允许用户自由处理来自一个或多个流的事件，并且提供一致性，高容错的状态管理。除此之外，用户可以注册事件事件并且处理基于时间的事件回调，从而实现更复杂的数据计算。 在实践中，许多应用程序并用不到上述描述的底层 api，而是采用更上层的 DataStream API 和 DataSet API 分别处理有界和无界数据流。这些 API 提供了通用的接口来处理数据，像用户指定的转换、连接、聚合、窗口、状态等操作，和 Spark 一样，在该层，可以比较轻便的编写一个数据处理的 pipeline。 datastream .flatMap(new FlatMapFunction>() {...}) .keyBy(0) .reduce(...) 其中需要注意的是数据类型的处理，在后面的 Data Types 中会讲到。由于 process function 已经和 DataStream API 集成，因此可以比较方便的使用底层 api 处理，比如状态管理与注册。对于批处理，Dataset API 也提供了更多的原语操作，比如 loop/iterations。 Table API 是基于表格的声明式 DSL，类似于 Spark 的 DataSet API。它遵循关系模型，每张表格都有自己的 schema，并且和关系数据库一样，提供了 select、join、group by 等基本查询。Table API 并不实现具体代码的逻辑，而是告诉程序该做什么，实际上就是屏蔽掉了 map，reduce function 的实现，只声明查询，聚合等操作。尽管 Table API 提供了 udf 的能力，使用起来也相对简洁，但是抽象和表现能力不如 DataStream API。除此之外，Table API 在真正分布到机器上去执行时，会经过解析，优化，将 RBO 规则应用到语法树上，从而得到相对最优的执行计划。 Table API 和 DataStream API 之间是可以互相转化的。 最高层 API 就是 SQL 了，在语义和表达式解析上都同 Table API 一样，但是不是 DSL，而是完全的 SQL 查询，就像下面这样： CREATE TABLE employee_information ( emp_id INT, name VARCHAR, dept_id INT ) WITH ( 'connector' = 'filesystem', 'path' = '/path/to/something.csv', 'format' = 'csv' ); select * form employee_information limit 1 1.1.3. 思考 1.以 api 来看， Flink 和 Spark 有啥区别？ 单看 Flink 提供的 API 的话，和 Spark 是异曲同工。Flink 对 Dataflow Model 的实现更好，状态和检查点的管理集成的更好，但是流批 api 不统一，一般在做数据初始化的时候要开发两套程序，对于现在仍在大多数场景下使用的 lambda 架构不是很友好。Spark 贵在 api 统一，2.x 之后的 structed streaming 对于流处理的支持更上一层，但是状态的管理和自动 checkpoint 的支持不太好，并且目前 Spark 的开发重心在 Spark ML。 2.Flink 和 Spark 分别适用于什么场景？ 就生产上来看的话，对于无状态的数据流处理，峰值打到 10000qps 的情况下。Flink 和 Spark streaming 的处理都是 hold 住的，时延都在可接受范围内，并且由于链路比较长，binlog 产生到收集就大概 15s，程序处理大概 5s，整个链路基本上控制在 30s 内。所以对于一些 etl 而非聚合指标任务来说，采用这两个都是没问题的。 对于有状态的数据流处理，Flink 显然集成更好，并且容错机制更完善。 目前国内在推 Flink，所以各家基本上都在从 Spark 切换到 Flink，并且基于 Flink 搭建内部实时开发平台。 3.用的最多的 api 是什么？ 个人理解是 SQL，SQL 作为一种早就普及的标准化语言，使用起来成本低，SQL+UDF一般也能覆盖到 %80 的业务场景，大多数解析计算都是无状态的，使用 SQL API 开发更便捷，更高效，尤其是内部基建比较好的公司，通过统一的实时开发平台开发这种 etl 程序，熟悉之后，几分钟之内就可以上线一个实时流任务，将 kafka 中数据写入到下游。 By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"DataStream API/":{"url":"DataStream API/","title":"DataStream API","keywords":"","body":" 导航信息 概览 流/批的运行模式 事件时间 状态容错与管理 Data Sources Operators UDF By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"DataStream API/overview.html":{"url":"DataStream API/overview.html","title":"概览","keywords":"","body":"1.1.1. DataStream API1.1.2. 什么是 DataStream1.1.3. Flink 程序剖析1.1.4. 示例1.1.5. Data Sources1.1.6. Data Sinks1.1.7. Iterations1.1.8. 执行参数1.1.9. 调试1.1.10. 思考1.1.1. DataStream API DataStream API 用来对数据流进行摄入，过滤，更新，聚合等一系列操作。数据来源可以是多种多样的（比如静态文件，消息队列，socket 流等）。数据流处理的结果流向 sink，比如写数据到文件或者输出到标准输出。Flink 程序跑在多种上下文环境中，standalone 或者嵌入在其它程序中。程序可以执行在一个本地 JVM 进程，也可以运行在拥有多台机器的集群上。 1.1.2. 什么是 DataStream DataStream API 得名于 Flink 内部实现的数据集合类-DataStream。你可以认为 DataStream 是一种可以包含重复数据的数据集合，这些数据可以是有界和无界的，其处理 api 是相同的。emmn，这里也说明 Flink 统一了流批 api，都可以使用 DataStream 来对数据进行操作。 DataStream 类似于 Java Collection，但是在一些核心方法上有所不同。DataStream 是不可变的，这意味着它一旦创建便不能更新，并且不能直接检查元素，只能依靠 DataStream API 提供的 operator 进行操作，这些 operator 也叫作 transformations。 1.1.3. Flink 程序剖析 Flink 程序通常由以下几部分组成： 创建执行环境，及 Execution Environment。 从外部数据源加载或者直接创建初始数据。 指定初始数据流后续的转换操作。 指定 sink，即计算结果该输出到什么地方。 触发程序的执行。 下面会详细的介绍这几步过程。DataStream 的核心类都可以在 org.apache.flink.streaming.api 中找到。 StreamExecutionEnvironment 是 Flink 程序的入口，可以通过以下方法创建 getExecutionEnvironment() createLocalEnvironment() createRemoteEnvironment(String host, int port, String... jarFiles) 通常，你只需要使用 getExecutionEnvironment()，Flink 会根据上下文创建合适的执行环境：如果你在 IDE 中运行，那么就会创建 local environment 用以在本机执行程序。如果你将程序打成 jar 包并且通过命令行提交，那么 Flink 集群管理器会执行 main 方法并调用 getExecutionEnvironment()，返回一个集群下的执行环境。 env 中的一些方法可以用来指定 source。如果要读取文件的话可以采用以下代码： final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream text = env.readTextFile(\"file:///path/to/file\"); 这会创建 DataStream，接下来就可以在上面应用一些转换操作，得到新的 DataStream。比如下面对每行数据转 int。 DataStream input = ...; DataStream parsed = input.map(new MapFunction() { @Override public Integer map(String value) { return Integer.parseInt(value); } }); 一旦 DataStream 完成最终的操作，就可以 sink 到外部系统中。 writeAsText(String path) print() 在完成以上程序后，需要通过 env.execuet() 方法触发程序的执行。execute 是同步的，需要等待程序执行完成返回最终结果。如果你不想等待 job 的执行就退出，那么可以使用 executeAysnc() 方法，它会返回一个 jobClient，用以与程序通信。比如，下面的代码使用 executeAysnc 来实现 execute 一样的功能。 final JobClient jobClient = env.executeAsync(); final JobExecutionResult jobExecutionResult = jobClient.getJobExecutionResult().get(); Flink 程序都是惰性执行的：当程序的 main 方法被执行时，数据的加载和转换并不是立刻发生的，而是先生成 job graph。当执行调用 execute 时，这些操作才被真正的执行。惰性执行可以使我们构建复杂程序作为一个完整单元执行。 1.1.4. 示例 下面是一个 wordcount 程序，首先命令行启动 nc -lk 9999，在启动 Flink 程序，命令行输入单词，就会看到程序的 标准输出。 import org.apache.flink.api.common.functions.FlatMapFunction; import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.util.Collector; public class WindowWordCount { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream> dataStream = env .socketTextStream(\"localhost\", 9999) .flatMap(new Splitter()) .keyBy(value -> value.f0) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .sum(1); dataStream.print(); env.execute(\"Window WordCount\"); } public static class Splitter implements FlatMapFunction> { @Override public void flatMap(String sentence, Collector> out) throws Exception { for (String word: sentence.split(\" \")) { out.collect(new Tuple2(word, 1)); } } } } 1.1.5. Data Sources Source 是指程序能够从哪里读取数据，你可以使用 StreamExecutionEnvironment.addSource(sourceFunction) 来添加自定义数据源。Flink 内部已经定义了一些简单数据源，但是还是经常需要通过 SourceFunction 来自定义实现非并行数据源，通过 ParallelSourceFunction 和 RichParallelSourceFunction 来定义并行数据源。 从 StreamExecutionEnvironment 可以访多个预定义的数据源。 基于文件的： readTextFile(path) - 读取文本文件，符合 TextInputFormat 格式的文件，逐行读取并作为字符串返回。 readFile(fileInputFormat, path) - 指定 fileInputFormat 读取文件。 readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - 这是前两个方法内部调用的方法，按照给定的 fileInputFormat 读取 path 路径下的文件。如果 watchType=FileProcessingMode.PROCESS_CONTINUOUSLY，则会定时监控是否有新数据进来并处理。如果 watchType=FileProcessingMode.PROCESS_ONCE，则程序只会读取当前文件内容，处理完成后就会退出。pathFilter 用于排除一些文件。 执行过程： Flink 会启动两个 subtask 来执行，及目录监控和数据读取。监控 task 是非并行的，读取 task 会切分成多个任务来并行执行，后者的并行度等于 job 的并行度。监控任务的作用是扫描文件（根据 watchType 扫描一次或者多次），找到待处理的文件，根据配置划分成各个分片发送给下游 reader 读取。每个分片只能被一个个 reader 读取。一个 reader 可以读取多个分片，只不过是串行的。 注意事项 如果 watchType=FileProcessingMode.PROCESS_CONTINUOUSLY，文件被更改会造成整个文件内容被重新处理，这破坏了精确处理一次的语义。向文件后面追加数据也会造成重复处理整个文件。 如果 watchType=FileProcessingMode.PROCESS_ONCE，只会扫描一次数据源就会断开与其的连接，这会造成后续没有 checkpoint 的产生，在任务失败重新恢复时会从一个相对较老的 checkpoint 恢复，因此 failover 的时间会变长。 基于 socket： socketTextStream，读取 socket 流。 基于集合的： fromCollection(Collection) - 从 Java.util.Collection 中创建 datastream，集合中的元素类型必须是相同的。 fromCollection(Iterator, Class) - 从迭代器中创建 datastream，Class 表示迭代器返回的元素类型。 fromElements(T ...) - 从给定的序列中创建 datastream，序列中的元素要求相同类型。 fromParallelCollection(SplittableIterator, Class) - 从并行的迭代器中创建 datastream，Class 表示迭代器返回的元素类型。 generateSequence(from, to) - 创建指定范围内的序列，依据此序列创建 datastream。 自定义： addSource - 实现一个新的 sourceFunction，比如 Kafka，就是 addSource(new FlinkKafkaConsumer<>(...)).。这里需要阅读下 connector 章节。 1.1.6. Data Sinks Data sinks 消费 DataStreams 并将他们输出到文件，sockets，外部系统或者标准输出中。Flink 已经内置了一些 sink。 writeAsText() / TextOutputFormat writeAsCsv(...) / CsvOutputFormat print() / printToErr() - 如果是并行输出，会输出每个 subtask 的标号。 writeUsingOutputFormat() / FileOutputFormat writeToSocket addSink 需要注意类似 write*() 的方法往往用于调试。它们不参与 Flink 的 checkpoint，这意味着这些方法只支持至少处理一次语义。数据写入到目标系统取决于使用的 OutputFormat，可能是打满 buffer 后在落盘，因此数据不会立刻刷入到目标系统中。同样的，如果任务失败，存在着丢数的风险。 要使用具有可靠性，精确处理一次语义特性的话，可以使用 StreamingFileSink。使用 addSink 方法实现的 sinks 也会参与 checkpoint。 1.1.7. Iterations 这里主要讲的是 Iterative Stream，一开始没太搞明白，直接看下面这张图会更清晰点。迭代流需要实现何时将流返回，何时将流发往下游。 1.1.8. 执行参数 StreamExecutionEnvironment 可以设置 Flink 程序运行时的一些配置信息。可以查看 execution configuration 了解更多。 容错管理 State & Checkpointing 描述了如何开启并配置 Flink checkpoint。 延迟控制 数据流在传输中为了避免不必要的网络流量，都会先缓存数据。在 Flink 配置文件中可以设置该 buffer 的大小，这样可以提高程序吞吐量，但是在上游数据 qps 不高的情况下，会增大数据的延迟。因此，为了衡量吞吐量和延时，可以设置 buffer 填满的最大等待时间，如果时间到了，buffer 仍然未被填满，那么就不在等待而是直接发送数据到下游。默认的超时时间是 100ms。 LocalStreamEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(); env.setBufferTimeout(timeoutMillis); env.generateSequence(1,10).map(new MyMapper()).setBufferTimeout(timeoutMillis); 为了最大化吞吐量，可以使用 setBufferTimeout(-1) ，在 buffer 未被填满的情况下不会像下游发送数据。为了最大化延迟，可以使用 setBufferTimeout(0)，但是会导致严重的性能问题。 1.1.9. 调试 在将程序部署到正式分布式环境前肯定需要调试验证程序的可用性，这里讲的就是如何在本地简单调试。但是其实在工作中，往往受限于各种代理问题，无法本地连上某些服务，比如 Kafka 等，因此往往需要走 CI 打包到开发机上运行在调试。以下这些方法个人认为比较适合做实验，就像 Spark 的 Seq(...).toDF() 等。 Local Execution Environment 创建本地执行环境，类似 Spark Local，就是 JM，TM 啥的都在一个进程里。 final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(); DataStream lines = env.addSource(/* some source */); // build your program env.execute(); Collection Data Sources 从 java 集合中创建 datastream，方便用于测试数据。 final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(); // Create a DataStream from a list of elements DataStream myInts = env.fromElements(1, 2, 3, 4, 5); // Create a DataStream from any Java collection List> data = ... DataStream> myTuples = env.fromCollection(data); // Create a DataStream from an Iterator Iterator longIt = ... DataStream myLongs = env.fromCollection(longIt, Long.class); 需要注意集合数据类型需要实现对应的序列化方法。Collection Data Sources 的并行度是 1。 Iterator Data Sink 如果是为了调试的话，感觉输出到标准输出或者文件都挺好的。官网这里建议可以使用迭代器，将结果收集到一个迭代器中，在做后续处理，不管是查询还是输出。 import org.apache.flink.streaming.experimental.DataStreamUtils DataStream> myResult = ... Iterator> myOutput = DataStreamUtils.collect(myResult) 1.1.10. 思考 1. 创建执行环境时的上下文指的是啥？ 在 org.apache.flink.streaming.api.environment 可以找到入口函数，然后就开始点点点看源码中的获取过程。 public static StreamExecutionEnvironment getExecutionEnvironment(Configuration configuration) { return Utils.resolveFactory(threadLocalContextEnvironmentFactory, contextEnvironmentFactory) .map(factory -> factory.createExecutionEnvironment(configuration)) .orElseGet(() -> StreamExecutionEnvironment.createLocalEnvironment(configuration)); } 2. setBufferTimeout 方法是 operator 级别还是 env 级别的？ 这块需要测试下，以 binlog 为例，统计到某个 operator 时的端到端延迟。 By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"DataStream API/execution_mode.html":{"url":"DataStream API/execution_mode.html","title":"流/批的运行模式","keywords":"","body":"1.1.1. 执行模式（流/批）1.1.2. 何时使用 Datastream 的批执行模式1.1.3. 配置批执行模式1.1.4. 流/批下的 Execution Behavior1.1.5. 思考1.1.1. 执行模式（流/批） Datastream API 支持流/批两种运行模式。最常见的就是实时数据流处理，即 STREAMING mode。除此之外，Flink 在 Datastream API 上也实现了 BATCH mode，类似 MR，Spark 的批处理，这适用于有界数据流处理，常用于状态初始化。 在 Datastream API 上统一流/批处理意味着对于相同的有界输入，不管使用何种模式，程序返回的最终结果是一致的，只不过可能底层执行过程有所不同。 使用 BATCH mode 时，Flink 对于有界数据流的处理会采用不同的执行策略（比如 join ），相比流处理，批处理的调度和任务恢复会更简单高效，这里可以联想下 Spark 的批处理。 1.1.2. 何时使用 Datastream 的批执行模式 BATCH mode 只能用于有界数据流处理。而 STREAMING mode 可以处理有界和无界数据流。通常，我们使用批模式来处理有界数据，因为这种方式更加高效。 有些时候，我们需要初始化无界数据流的状态。此时，可以先采用 STREAMING mode 处理有界数据，生成 savepoint。在通过 savepoint 启动无界数据流任务，从而完成任务状态的衔接。 1.1.3. 配置批执行模式 执行模式可以通过 execution.runtime-mode 设置进行配置，存在以下三种取值： STREAMING：Datastream 默认模式 BATCH - Datastream 批模式 AUTOMATIC - 根据数据源的有界性自动判断使用何种运行模式。 也可以通过 bin/flink run ... 的命令行参数进行配置，或者在创建 StreamExecutionEnvironment 以编程方式进行配置。 命令行提交任务时配置 $ bin/flink run -Dexecution.runtime-mode=BATCH examples/streaming/WordCount.jar 在 env 中配置 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setRuntimeMode(RuntimeExecutionMode.BATCH); 建议在命令行中配置，因为这样程序拓展性更好，避免了同一套逻辑在流/批之间的切换。 1.1.4. 流/批下的 Execution Behavior 本小节主要讲下 Datastream API 在流/批模式下的一些不同。 任务调度和 shuffle Flink 程序包含多个 operator，这些 operator 从前往后按照某种模式连接起来就构成了数据处理的 pipeline。那么如何在多个进程乃至机器之间调度这些 operator？各个 operator 间又是怎样进行数据交换的呢？ 在执行过程中，许多 operator 可以 chain 在一起，旨在降低资源的过多使用，提高性能。一个 operator 或者多个 chain 在一起的 operator组成 Flink 程序的最小调度单元。 任务调度和网络数据交换在流/批模式下是不同的，对于有界数据流的处理，可以使用更多的数据结构和策略，因而相比 STREAMING mode 处理有界数据流更高效。下面会使用一个例子说明这些不同点。 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource source = env.fromElements(...); source.name(\"source\") .map(...).name(\"map1\") .map(...).name(\"map2\") .rebalance() .map(...).name(\"map3\") .map(...).name(\"map4\") .keyBy((value) -> value) .map(...).name(\"map5\") .map(...).name(\"map6\") .sinkTo(...).name(\"sink\"); 对于一对一连接的 operator，像 map()、filter() 这些可以直接将数据发往下游 operator，因此这些 operator 可以 chain 在一起。这也意味着这些 operator 间不会产生 shuffle。而像 keyBy()、reblance() 这些 operator，往往需要将数据发往不到不同的节点（这取决于任务的本地化级别），从而会产生网络数据交换。 上面的示例会生成三个 task Task1: source、map1、map2 Task2: map3、 map4 Task3: map5、map6、sink 在 task1 和 taks2，task2 和 task3 之间会产生网络 shuffle。下面是该 job 逻辑图的简单描述 流模式 在流模式下，所有的任务都要一直运行，这使得 Flink 程序一接到数据就可以流经整条 pipeline 快速处理。这也意味着 TaskManager 需要分配足够的资源保证同时启动所有的 task 并可以正常运行。 数据交换也是基于 pipeline 的，数据会被立刻发往下游，只不过在网络侧会存在缓冲区，旨在提高吞吐量。在流式处理中，shuffle 的中间结果是不会落盘的。 批模式 在批模式下，任务会被划分成几个阶段，从前往后逐一实行（当然，毫不相关的前置任务也是可以并行执行的）。我们能做到这些是因为数据流是有界的，我们可以处理完一个阶段的所有数据后在接着处理。像上面的例子会被划分成三个 stage，这里 stage 的划分和 Spark 是一样的，碰到 shuffle dependency 就断开 pipeline。 不同于流模式下直接发送数据，批模式会先 shuffle write 将数据落盘，下一阶段在进行 shuffle read 读取上游落盘后的数据。这虽然会增大处理延迟，但是对于批处理是有益的。 任务失败时，不必重启所有的 task，而是从上次缓存的中间结果处开始。 更少的资源使用，不必一开始就启动所有的 task。 TaskManager 会保留中间结果，直到下游 task 开始读取。并且 TM 会在空间允许的情况下一直保留该中间结果，方便某个阶段任务失败重启。 状态管理 在流模式下，Flink 通过状态后端来控制状态的存储和 checkpoint 的生成。 在批模式下，配置的状态后端被忽略。数据流可以按照 key 被顺序处理，因此同一时刻只有一个 key 的状态，在处理下一个 key 时，上一个 key 的状态会被清理掉。 处理顺序 在流模式下，不考虑数据的顺序，在数据到达后就会根据程序逻辑尽可能快的处理。 在批模式下，有些 operator 的操作是可以保证顺序的，这取决于系统执行某些操作时的策略，也可能是 shuffle，任务调度的副作用。 一般分为三种输入，批模式下会按照如下顺序处理 broadcast input: input from a broadcast stream。 regular input: input that is neither broadcast nor keyed。 keyed input: input from a KeyedStream。 事件时间/ watermark Flink 使用 watermark 机制来处理无序数据流。在批模式下，数据流是有界的，我们可以确定某些时间前的数据都已经到达了，因此可以直接指定一个最大的 watermark 值，计算会在最后被触发，注册的 WatermarkAssigners 和 WatermarkGenerators 会被忽略。 处理时间 处理时间是程序处理数据那一刻的机器时间，基于此机制的计算是无法保证幂等的，因此同样的数据在两次处理过程中的处理时间可能是不同的。 尽管如此，在流模式下使用处理时间仍然是有些场景的。在数据延迟乱序可控的情况下，处理时间的一小时相当于事件时间的一小时，使用处理时间计算会更快的被触发。 在批模式下，和事件时间的处理是一样的，在最后的输入后触发整体的计算。 任务恢复 在流模式下，Flink 使用 checkpoint 实现容错管理，任务失败后，会从 checkpoint 恢复并重新启动调度所有的 task 。 在批模式下，任务失败后，Flink 会尝试回溯到上一个阶段重新开始，默认只有失败的上游 task 会被重启，从而减少任务恢复时间。 1.1.5. 思考 1. Datastream 在批模式下和 Dataset 有啥区别呢？ By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"DataStream API/event_time.html":{"url":"DataStream API/event_time.html","title":"事件时间与 Watermark","keywords":"","body":"1.1.1. WatermarkStrategy1.1.2. 处理空闲数据源1.1.3. WatermarkGenerators1.1.4. WatermarkStrategy && Kafka Connector1.1.5. operator 是如何处理 watermark 的1.1.6. 内置的 WatermarkGenerators1.1.7. 思考1.1.1. WatermarkStrategy 为了程序能够基于事件时间处理，Flink 需要准确知道事件发生的时间戳，这意味着数据流中的每个元素需要注册自己的事件时间戳。一般会使用 TimestampAssigner 从数据中的某些字段来提取事件时间。 事件时间的提取影响 watermark 的生成（watermark 代表着事件处理的进度）。我们可以使用 WatermarkGenerator 来生成 watermark。 emmn，貌似直接看上面两句话根本不知道再讲啥。但是 WatermarkStrategy 就是由 TimestampAssigner 和 WatermarkGenerator 组成的。watermark 策略暴露出来的接口就是实现以上两个方法。Flink 提供了内置的 WatermarkStrategy，也允许自定义。 public interface WatermarkStrategy extends TimestampAssignerSupplier, WatermarkGeneratorSupplier{ /** * Instantiates a {@link TimestampAssigner} for assigning timestamps according to this * strategy. */ @Override TimestampAssigner createTimestampAssigner(TimestampAssignerSupplier.Context context); /** * Instantiates a WatermarkGenerator that generates watermarks according to this strategy. */ @Override WatermarkGenerator createWatermarkGenerator(WatermarkGeneratorSupplier.Context context); } 正如上面提到的，很多时候我们不需要自己实现这个接口，只需要调用内部策略。比如下面这样，使用有界无序的 watermark + 基于 lambda 函数实现的时间注册器。 WatermarkStrategy .>forBoundedOutOfOrderness(Duration.ofSeconds(20)) .withTimestampAssigner((event, timestamp) -> event.f0); watermark 可以在读取数据源或者进行后续某些操作后注册。通常我们会选择在读取数据源时注册，这样产生的 watermark 更加准确，也会考虑到初始分区，分片的影响。直接在源端指定 WatermarkStrategy 有时候需要在数据源实现一些特殊的接口，比如 Kafka，这个在下面 WatermarkStrategy && Kafka Connector 会提到。 1.1.2. 处理空闲数据源 在实时数据流处理过程中，数据源往往是经过分区分片的，比如 Kafka，某个分区在一段时间内没有写入数据，这个分区就是一个空闲数据源，读取该分区的 task 在这段时间内是空跑的。空闲数据源带来一个问题，就是 watermark 也不会随之更新。此时，如果其他分区仍然有数据写入，对于下游有多个输入的 operator 来说，watermark 仍然停留在没有数据写入的分区最后一次产生的时间，不会处理其它分区后续写入的数据。 为了解决上述问题，我们需要将某段时间内没有写入的数据源分区标记为空闲，在 watermark 传往下游时，空闲数据源的 watermark 暂时不参与计算。WatermarkStrategy 提供了方便的方法来做这件事情。 // 如果超过 1min 没有写入数据，就标记为空闲 WatermarkStrategy .>forBoundedOutOfOrderness(Duration.ofSeconds(20)) .withIdleness(Duration.ofMinutes(1)); 1.1.3. WatermarkGenerators TimestampAssigner 方法比较简单，就是从事件数据某些字段中提取转换出事件发生的时间戳，我们往往不需要关注太多。WatermarkGenerator 相对比较复杂，下面是其定义的接口 /** * The {@code WatermarkGenerator} generates watermarks either based on events or * periodically (in a fixed interval). * * Note: This WatermarkGenerator subsumes the previous distinction between the * {@code AssignerWithPunctuatedWatermarks} and the {@code AssignerWithPeriodicWatermarks}. */ @Public public interface WatermarkGenerator { /** * Called for every event, allows the watermark generator to examine * and remember the event timestamps, or to emit a watermark based on * the event itself. */ void onEvent(T event, long eventTimestamp, WatermarkOutput output); /** * Called periodically, and might emit a new watermark, or not. * * The interval in which this method is called and Watermarks * are generated depends on {@link ExecutionConfig#getAutoWatermarkInterval()}. */ void onPeriodicEmit(WatermarkOutput output); } watermark 生成有两种方式： periodic watermark 基于定时，定时生成会通过 onEvent() 获取每条数据的时间作为待选的 watermark。在框架调用 onPeriodicEmit() 时生成最终的 watermark。 punctuated watermark 基于规则（某些特殊事件触发），规则生成也是通过 onEvent() ，在获取到某些特殊事件后，直接生成 watermark，而不通过 onPeriodicEmit()。 基于定时的实现 定时生成器会监听到来的事件，定时触发 watermark 的生成（基于事件时间或者单纯的处理时间都可以）。定时触发的时间间隔可以通过 ExecutionConfig.setAutoWatermarkInterval(...) 来设置，到达时间触发时会调用 onPeriodicEmit() 方法，只要这次的 watermark 不为 null 并且大于上次生成的 watermark，就会成功生成。下面是两个例子： /** * This generator generates watermarks assuming that elements arrive out of order, * but only to a certain degree. The latest elements for a certain timestamp t will arrive * at most n milliseconds after the earliest elements for timestamp t. */ public class BoundedOutOfOrdernessGenerator implements WatermarkGenerator { private final long maxOutOfOrderness = 3500; // 3.5 seconds private long currentMaxTimestamp; @Override public void onEvent(MyEvent event, long eventTimestamp, WatermarkOutput output) { currentMaxTimestamp = Math.max(currentMaxTimestamp, eventTimestamp); } @Override public void onPeriodicEmit(WatermarkOutput output) { // emit the watermark as current highest timestamp minus the out-of-orderness bound output.emitWatermark(new Watermark(currentMaxTimestamp - maxOutOfOrderness - 1)); } } /** * This generator generates watermarks that are lagging behind processing time * by a fixed amount. It assumes that elements arrive in Flink after a bounded delay. */ public class TimeLagWatermarkGenerator implements WatermarkGenerator { private final long maxTimeLag = 5000; // 5 seconds @Override public void onEvent(MyEvent event, long eventTimestamp, WatermarkOutput output) { // don't need to do anything because we work on processing time } @Override public void onPeriodicEmit(WatermarkOutput output) { output.emitWatermark(new Watermark(System.currentTimeMillis() - maxTimeLag)); } } 基于规则的实现 基于规则的生成就是当遇到某些特殊事件时才会生成 watermark。 public class PunctuatedAssigner implements WatermarkGenerator { @Override public void onEvent(MyEvent event, long eventTimestamp, WatermarkOutput output) { if (event.hasWatermarkMarker()) { output.emitWatermark(new Watermark(event.getWatermarkTimestamp())); } } @Override public void onPeriodicEmit(WatermarkOutput output) { // don't need to do anything because we emit in reaction to events above } } 使用规则生成需要主要，有些场景下，连续到来的每个事件可能都会触发 watermark，由此产生过多的 watermark，造成下游的频繁计算，降低性能。 1.1.4. WatermarkStrategy && Kafka Connector Kafak 是分区内有序，全局乱序的，并且一般都是多线程消费。所以 watermark 的流转和 shuffle 是一样的，可以直接看下面的例子 FlinkKafkaConsumer kafkaSource = new FlinkKafkaConsumer<>(\"myTopic\", schema, props); kafkaSource.assignTimestampsAndWatermarks( WatermarkStrategy. .forBoundedOutOfOrderness(Duration.ofSeconds(20))); DataStream stream = env.addSource(kafkaSource); 1.1.5. operator 是如何处理 watermark 的 operator 在处理完由 watermark 触发的所有计算后，才会将 watermark 继续发往下游。同样的规则适用于 TwoInputStreamOperator，只不过此时发出去的 watermark 是多个输入中最小的。更多的细节可以查看源码中 OneInputStreamOperator#processWatermark, TwoInputStreamOperator#processWatermark1 和 TwoInputStreamOperator#processWatermark2 方法. 1.1.6. 内置的 WatermarkGenerators 单调递增生成 watermark、 单调递增即数据的到来是有序并且是增序的，这意味着当前的时间戳就可以当做 watermark，因为不会存在迟到数据。 WatermarkStrategy.forMonotonousTimestamps(); 固定延迟生成 watermark 使用这种方式有个前提，就是需要提前预估好数据流的延迟，否则数据容易产生误差。固定延迟意味着每次生成 watermark 都会往前倒退一段时间作为最终的 watermark 发往下游，超出固定延迟的数据将被丢弃，不会触发计算。 WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(10)); 1.1.7. 思考 1. 自己分别实现两种 watermark 的生成，观察一下 watermark 是如何触发计算的？ 可以参考测试代码 By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"DataStream API/datasource.html":{"url":"DataStream API/datasource.html","title":"Data Sources","keywords":"","body":" By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"DataStream API/operators.html":{"url":"DataStream API/operators.html","title":"Operators","keywords":"","body":" By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"DataStream API/udf.html":{"url":"DataStream API/udf.html","title":"UDF","keywords":"","body":" By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"State & Fault Tolerance/":{"url":"State & Fault Tolerance/","title":"状态容错与管理","keywords":"","body":" 导航信息 状态处理 状态广播 Checkpointing 状态查询 状态后端 状态容错 状态迁移与演化 自定义状态序列化器 By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"State & Fault Tolerance/work_with_state.html":{"url":"State & Fault Tolerance/work_with_state.html","title":"状态处理","keywords":"","body":"1.1.1. Keyed DataStream1.1.2. Using Keyed State1.1.3. Operator State1.1.4. Broadcast State1.1.5. Using Operator State1.1.6. 思考1.1.1. Keyed DataStream 如果你想使用 keyed state，那么就需要指定一个 key 字段用于状态与数据的分区。可以通过 keyBy(KeySelector) 方法来指定 key，这会返回 keyDataStream，用来后续在数据流上使用 keyed state。 KeySelector 接口的实现就是将每条记录当做输入，然后返回该条记录的 key，可以理解为业务处理逻辑上数据唯一标识。返回的 Key 可以是任意类型，但是一定是确定的，即输入相同的记录，得到的 key 应该是一样的。 Flink 的数据模型并不是基于键值对的，所以没必要将数据物理的处理成 key-value 的格式。Key 的概念是虚拟的，只是通过某些方法提取出记录的 key 作用于某些 operator 上（比如 group by 这些）。 下面是 KeySelector 的示例 // some ordinary POJO public class WC { public String word; public int count; public String getWord() { return word; } } DataStream words = // [...] KeyedStream keyed = words .keyBy(WC::getWord); Tuple Keys and Expression Keys Flink 还有两种声明 key 的方式，但是在新版本中已经不经常使用，使用 KeySelector 对于大多数场景更简单并且提供了更好的性能保证。 Tuple 数据的下标指定，比如很多示例中常见的 keyBy(0)。 表达式指定，即 expression。 1.1.2. Using Keyed State Keyed State 接口提供了对不同类型状态的访问。目前，Flink 提供了了以下几种状态原语： ValueState：该状态保存一个可以被更新和读取的值（作用域为输入元素的键，读取不同键值返回的结果是不一样的）。通过 update(T) 更新，T value() 读取。 ListState：该状态保存一个元素数组，可以添加元素和通过迭代器读取元素数组。通过 add(T) 或者 `addAll(List) 添加元素、Iterable get() 读取、update(List) 覆盖更新已经存在的状态。 ReducingState：该状态保存一个值，含义是该 key 下所有状态值的聚合。接口类似于 ListState，但是是通过 add(T) 添加元素，ReduceFunction 实现聚合计算。 AggregatingState：该状态保存一个值，含义是该 key 下所有状态值的聚合。与 ReducingState 不同的是，聚合值的类型可能和输入元素的类型不一样（比如输入一堆事件，通过事件的某些标记进行聚合）。通过 add(IN) 添加元素，AggregateFunction 实现聚合计算。 MapState：该状态保存一个 mapping 数组。可以向该状态中添加键值对和通过迭代器读取已经存在的 mapping。通过 put(UK, UV) 或者 putAll(Map) 添加元素、get(UK) 获取指定 key 的元素。可以分别使用 entry()、keys() 和 values() 访问迭代器检索键值对、键和值。使用 isEmpty() 检测状态是否为空。 所有类型的状态都有 clear() 方法，用来清空当前 key 的所有状态。 需要重点注意的是，状态操作暴露出来的接口仅用于与状态交互，与状态存储无关，状态可以存储再内部或者外部系统上。keyed state 的取值仅与调用时传入的 key 值有关，因此不同的 key 取回来的状态是不一样的。 要获得状态处理的句柄，需要声明 StateDescriptor，包含状态的名称（下面会讲到，存储的状态拥有一个唯一名称，可以理解为命名空间，可以通过该命名操作所有状态）、状态的类型，可选的用户自定义方法（比如 ReducingFunction）。根据你想取回来的状态类型，可以声明 ValueStateDescriptor、 ListStateDescriptor、AggregatingStateDescriptor、 ReducingStateDescriptor、 MapStateDescriptor。 状态通过 RuntimeContext 访问，所以只能应用于 rich functions。RichFunction 中的 RuntimeContext 通过以下方法访问状态： ValueState getState(ValueStateDescriptor) ReducingState getReducingState(ReducingStateDescriptor) ListState getListState(ListStateDescriptor) AggregatingState getAggregatingState(AggregatingStateDescriptor) MapState getMapState(MapStateDescriptor) 下面展示了一个示例，当某个 key 的数据量到达两个时，就计算平均值，并清空当前的状态，如此往复。 public class CountWindowAverage extends RichFlatMapFunction, Tuple2> { /** * The ValueState handle. The first field is the count, the second field a running sum. */ private transient ValueState> sum; @Override public void flatMap(Tuple2 input, Collector> out) throws Exception { // access the state value Tuple2 currentSum = sum.value(); // update the count currentSum.f0 += 1; // add the second field of the input value currentSum.f1 += input.f1; // update the state sum.update(currentSum); // if the count reaches 2, emit the average and clear the state if (currentSum.f0 >= 2) { out.collect(new Tuple2<>(input.f0, currentSum.f1 / currentSum.f0)); sum.clear(); } } @Override public void open(Configuration config) { ValueStateDescriptor> descriptor = new ValueStateDescriptor<>( \"average\", // the state name TypeInformation.of(new TypeHint>() {}), // type information Tuple2.of(0L, 0L)); // default value of the state, if nothing was set sum = getRuntimeContext().getState(descriptor); } } // this can be used in a streaming program like this (assuming we have a StreamExecutionEnvironment env) env.fromElements(f) .keyBy(value -> value.f0) .flatMap(new CountWindowAverage()) .print(); // the printed output will be (1,4) and (1,5) State TTL 状态不可能无限期保存下去，因此有了 TTL 的概念，过期的状态将会通过一些策略进行删除。集合类型的状态的每个元素都可以设置 TTL，这意味着 list 和 mapping 类型的状态中每个元素的清理是独立的。 使用 TTL 需要定义 StateTtlConfig，下面是示例： import org.apache.flink.api.common.state.StateTtlConfig; import org.apache.flink.api.common.state.ValueStateDescriptor; import org.apache.flink.api.common.time.Time; StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) .build(); ValueStateDescriptor stateDescriptor = new ValueStateDescriptor<>(\"text state\", String.class); stateDescriptor.enableTimeToLive(ttlConfig); StateTtlConfig 后面跟着几个必选项和可选项，newBuilder 是必选的，其参数定义了 TTL 的时间。 可选的 setUpdateType 定义了 TTL 的触发策略，提供了两个选择，默认是 OnCreateAndWrite StateTtlConfig.UpdateType.OnCreateAndWrite 在创建和写入时就会触发。 StateTtlConfig.UpdateType.OnReadAndWrite 在创建，写入，读取时都会触发。 setStateVisibility 用来设置到达 TTL 的状态但是未被清除的时候，读取是否要返回值。 StateTtlConfig.StateVisibility.NeverReturnExpired 不会返回过期状态 StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp 状态虽然过期但是未被清除，读取仍然会返回值。 Notes： 为了实现 TTL ，状态后端需要存储每个值修改的时间戳，这样会增加存储压力。Heap 状态后端会存储额外的 java 对象，包含对状态的引用和内存中的原始 long 值。RocksDB 会为每个状态元素添加 8 字节用于存储时间。 TTL 目前仅仅基于处理时间。 如果是从状态中恢复任务的话，需要关闭状态的 TTL，否则会引发任务失败和一些未知错误。 只要状态值的序列化支持 null，设置 TTL 的 map 类型的状态也可以支持处理 null 值。否则会抛出 NullableSerializer。 Python DataStream API 目前不支持 State TTL。 过期状态的清除策略 默认配置下，程序是不会读取到过期的状态的（不管此时状态是逻辑删除还是物理删除），如果状态后端支持的话，会启动一个后台进程定期收集过期状态进行删除。通过 StateTtlConfig 可以关闭该后台进程 import org.apache.flink.api.common.state.StateTtlConfig; StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .disableCleanupInBackground() .build(); 下面会介绍一些更细粒度的状态清理策略。一般 Heap 状态后端会进行增量更新，RocksDB 会通过压实策略进行清理。 Cleanup in full snapshot 在生成状态快照的时候触发状态的清理，这样做可以减少快照的大小。在当前实现下，本地状态不会被清理，但是前一个快照中过期状态已经被清理了。可以通过 StateTtlConfig 配置。 import org.apache.flink.api.common.state.StateTtlConfig; import org.apache.flink.api.common.time.Time; StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupFullSnapshot() .build(); 这种清理策略不适用于 RocksDB 上的增量快照。 Incremental cleanup 增量触发状态的清理，可以在访问状态或者处理记录时触发。基本实现是维持一个全局的状态元素迭代器，当清理被触发后，调用迭代器，遍历所有状态元素然后清理掉过期状态。同样是通过 StateTtlConfig 配置。 import org.apache.flink.api.common.state.StateTtlConfig; StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupIncrementally(10, true) .build(); 这里有两个参数，第一个参数代表每次触发清理时检查的状态条数，在访问状态的时候就会触发检查。第二个参数代表每次处理数据时是否需要触发清理策略。Heap Backend 默认是在状态访问时触发检查5条状态是否过期。 Notes： 如果没有状态访问或者数据流的处理，过期的状态会永远保存下去。 增量清理会增大处理时延。 目前仅对 Heap Backend 进行增量清理，不支持 RocksDB。 如果配置的 Heap Backend 是同步快照，则全局的迭代器会保留所有 key 的副本用来清理，因为其实现不支持并发修改。异步则没有此问题。 对于已经运行的 job，可以通过 StateTtlConfig 开启清理，例如从 savepoint 恢复作业时。 Cleanup during RocksDB compaction 使用 RocksDB 状态后端的话，会调用 Flink compaction filter 用于后台状态清理。compaction 意为压实，在大数据存储中经常会接触到，目的是对各种摄入任务的文件碎片进行合并压缩，减小碎片数量，提高存储查询的性能。这里的压实也是一样的，在对状态做合并压缩的过程中顺便对状态进行清理。 Flink compaction filter 会检查状态的时间并清除过期策略。 import org.apache.flink.api.common.state.StateTtlConfig; StateTtlConfig ttlConfig = StateTtlConfig .newBuilder(Time.seconds(1)) .cleanupInRocksdbCompactFilter(1000) .build(); 这种策略会在每次处理一定数量的状态元素后从 Flink 查询当前时间戳，用于检查过期时间。检查条数的配置就是 cleanupInRocksdbCompactFilter 的参数。RocksDB 采用这种策略的默认配置是每处理 1000 条数据就查询更新时间戳。emmn，我此处描述的其实非常不清晰。。 Notes： 显而易见，在 compaction 期间进行状态的清理会降低 compaction 的效率。程序必须获取上次查询的时间戳对存储的每个 key 的状态判断是否过期。对于 list，map 类型的状态，还要对状态存储中的每个元素进行检查。 1.1.3. Operator State Operator State 可以认为是绑定并行 operator 中的一个实例的状态。最明显的例子是 Kafka，在从 Kafka 源读取数据时，一般会根据 paratition 的数量设置并行度，这意味着读取数据源的 opertor 是并行多个实例的，这里的 Operator State 指的就是每个实例读取的 partition 及其 offset。 Operator State 接口支持在并行度发生改变时重新分配状态。分配策略有几种不同的方案，这个后面会讲到。 在典型的有状态数据流处理中，一般不需要 Operator State。Operator State 仅仅是一种特殊的状态类型，在 source 或者 sink 中实现，用于没有明确分区 key 的场景。 1.1.4. Broadcast State Broadcast State 是一种特殊的 Operator State。主要用于一条数据流中的数据需要广播到下游所有 task 的场景，即每个 task 保留的状态是一致的。一个例子是关于事件模式的匹配，吞吐量比较小的事件规则流作为状态广播到事件流，事件流中处理每条事件筛选出符合规则的用户行为。 Broadcast State 与 Operator State 主要有以下几点不同 it has a map format, it is only available to specific operators that have as inputs a broadcasted stream and a non-broadcasted one, and such an operator can have multiple broadcast states with different names. 1.1.5. Using Operator State 继承 CheckpointedFunction 来使用 Operator State。 CheckpointedFunction CheckpointedFunction 提供非 keyed state 状态的访问，需要实现以下两种方法 void snapshotState(FunctionSnapshotContext context) throws Exception; void initializeState(FunctionInitializationContext context) throws Exception; 当执行 checkpoint 时，snapshotState 就被调用。对应的是，在每次执行用户自定义函数或者从上个检查点恢复用户自定义函数执行时，会调用 initializeState()。这样看来，initializeState 不仅仅是初始化状态的地方，也可以包含恢复逻辑。 当前，支持 list 类型的 operator state。这种状态应该是一个可序列化对象的列表，彼此独立，这也是可以重新分配状态的基础，换句话说，这些可序列化对象是重新分配的最小粒度。基于此，定义了以下两中重分配策略： 均匀分配：每个 operator 拥有一个状态元素列表，整个状态可以看做是所有 operator 列表的组合。在重新分配状态时，该列表被平均分为与并行运算符一样多的子列表。每个 operator 都得到一个子列表，可能是空，也可能包含一个或者多个元素。例如，如果并行度为 1，operator state 中包含元素 element1 和 element2，当并行度增加到 2 时，element1 可能会在 operator 实例 0 中结束，而 element2 将转到 operator 实例 1。 合并分配：每个 operator 拥有一个状态元素列表，整个状态可以看做是所有 operator 列表的组合。在重新分配状态时，每个 operator 获取状态的所有元素。当状态是高基数时，不建议使用，因为 checkpoint 元数据将存储每个列表条目的偏移量，这可能导致 RPC 帧大小或内存溢出。 下面是一个均匀分配的例子，先缓存一批数据在 sink 出去。在创建 checkpoint 时保存 bufferedElements 的状态。在从 checkpoint恢复时，重放 bufferedElements。 public class BufferingSink implements SinkFunction>, CheckpointedFunction { private final int threshold; private transient ListState> checkpointedState; private List> bufferedElements; public BufferingSink(int threshold) { this.threshold = threshold; this.bufferedElements = new ArrayList<>(); } @Override public void invoke(Tuple2 value, Context contex) throws Exception { bufferedElements.add(value); if (bufferedElements.size() == threshold) { for (Tuple2 element: bufferedElements) { // send it to the sink } bufferedElements.clear(); } } @Override public void snapshotState(FunctionSnapshotContext context) throws Exception { checkpointedState.clear(); for (Tuple2 element : bufferedElements) { checkpointedState.add(element); } } @Override public void initializeState(FunctionInitializationContext context) throws Exception { ListStateDescriptor> descriptor = new ListStateDescriptor<>( \"buffered-elements\", TypeInformation.of(new TypeHint>() {})); checkpointedState = context.getOperatorStateStore().getListState(descriptor); if (context.isRestored()) { for (Tuple2 element : checkpointedState.get()) { bufferedElements.add(element); } } } } Stateful Source Functions source 相比其它 operator 更需要注意，一般需要锁机制做到原子更新保证精确一次处理语义。 public static class CounterSource extends RichParallelSourceFunction implements CheckpointedFunction { /** current offset for exactly once semantics */ private Long offset = 0L; /** flag for job cancellation */ private volatile boolean isRunning = true; /** Our state object. */ private ListState state; @Override public void run(SourceContext ctx) { final Object lock = ctx.getCheckpointLock(); while (isRunning) { // output and state update are atomic synchronized (lock) { ctx.collect(offset); offset += 1; } } } @Override public void cancel() { isRunning = false; } @Override public void initializeState(FunctionInitializationContext context) throws Exception { state = context.getOperatorStateStore().getListState(new ListStateDescriptor<>( \"state\", LongSerializer.INSTANCE)); // restore any state that we might already have to our fields, initialize state // is also called in case of restore. for (Long l : state.get()) { offset = l; } } @Override public void snapshotState(FunctionSnapshotContext context) throws Exception { state.clear(); state.add(offset); } } 1.1.6. 思考 1.自定义状态可以用来做什么？ 实现一些内置聚合函数无法做到的逻辑。实现程序恢复。可以参考测试代码 By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"State & Fault Tolerance/broadcast_state.html":{"url":"State & Fault Tolerance/broadcast_state.html","title":"状态广播","keywords":"","body":"1.1.1. The Broadcast State Pattern1.1.2. Provided APIs1.1.3. BroadcastProcessFunction and KeyedBroadcastProcessFunction1.1.4. Important Considerations1.1.5. 思考1.1.1. The Broadcast State Pattern 本节主要用来学习如何在实践中应用状态广播。需要预先了解下 Stateful Stream Processing 相关的概念。 1.1.2. Provided APIs 为了讲解相关的 API，我们会从一个例子开始，逐步展示 Broadcast State 的相关作用和功能。假设存在一个 item 流，里面的数据具有不同的颜色和形状属性，我们希望找到符合某种特定模式的 item 对，比如一个矩形后跟一个三角形。而这种特定模式又会随着时间而演变。 在这个例子中，第一条流包含具有颜色和形状属性的 item，第二条流中包含特定的匹配模式，即规则。 从 Items 流开始，因为我们需要相同颜色的成对数据，故需要按照颜色 keyBy 分组，以保证相同颜色的数据流向同一个 task。 // key the items by color KeyedStream colorPartitionedStream = itemStream .keyBy(new KeySelector(){...}); 接下来看规则流（即 pattern 模式流），该流中包含的数据需要被广播到所有下游 task 上，并在本地存储它们（实际上是在内存）以保证规则可以应用到源源不断的 items 流上。大概的步骤是这样 广播规则，每个并行示例存储规则到状态中。 通过创建 MapStateDescriptor 来访问本地已经存储的规则状态。 // a map descriptor to store the name of the rule (string) and the rule itself. MapStateDescriptor ruleStateDescriptor = new MapStateDescriptor<>( \"RulesBroadcastState\", BasicTypeInfo.STRING_TYPE_INFO, TypeInformation.of(new TypeHint() {})); // broadcast the rules and create the broadcast state BroadcastStream ruleBroadcastStream = ruleStream .broadcast(ruleStateDescriptor); 最后，为了在 Items 流上应用广播的规则，我们需要 连接两条流，即 connect stream 定义我们自己的规则匹配逻辑 连接两条流的时候只能在非广播流上（在上面的例子中就是 Items 流）调用 connect 方法，这会返回一个 BroadcastConnectedStream 对象，我们可以通过该对象调用 process 函数，从而实现规则匹配逻辑，process 函数底层调用的类型取决于非广播流的的类型。 KeyedStream -> KeyedBroadcastProcessFunction none-KeyedStream -> BroadcastProcessFunction DataStream output = colorPartitionedStream .connect(ruleBroadcastStream) .process( // type arguments in our KeyedBroadcastProcessFunction represent: // 1. the key of the keyed stream // 2. the type of elements in the non-broadcast side // 3. the type of elements in the broadcast side // 4. the type of the result, here a string new KeyedBroadcastProcessFunction() { // my matching logic } ); 1.1.3. BroadcastProcessFunction and KeyedBroadcastProcessFunction 和 CoProcessFunction 一样，广播状态相关的两个 processFunction 也实现了两个方法。processBroadcastElement() 用来处理规则流，processElement() 用来处理事件流，下面是这两个方法的签名： public abstract class BroadcastProcessFunction extends BaseBroadcastProcessFunction { public abstract void processElement(IN1 value, ReadOnlyContext ctx, Collector out) throws Exception; public abstract void processBroadcastElement(IN2 value, Context ctx, Collector out) throws Exception; } public abstract class KeyedBroadcastProcessFunction { public abstract void processElement(IN1 value, ReadOnlyContext ctx, Collector out) throws Exception; public abstract void processBroadcastElement(IN2 value, Context ctx, Collector out) throws Exception; public void onTimer(long timestamp, OnTimerContext ctx, Collector out) throws Exception; } 两个 process 方法的不同之处在于 processElement 是 ReadOnlyContext，而 processBroadcastElement 是 Context。 上下文的作用一般如下 （以下简称 ctx） 访问广播状态，即 broadcast state。ctx.getBroadcastState(MapStateDescriptor stateDescriptor) 获取数据的事件时间：ctx.timestamp() 获取当前的 watermark：ctx.currentWatermark() 获取当前的处理时间： ctx.currentProcessingTime() 旁路输出数据：ctx.output(OutputTag outputTag, X value) getBroadcastState() 获取到的状态实际上就是广播状态，在上述例子中是 ruleStateDescriptor。 广播流具有对广播状态的读写权限，而非广播流只有读权限。这样做的原因是在应用广播状态时，Flink 没有跨任务通信。为了保证在各个并行实例间广播状态的一致性，我们只为广播端提供读写访问权限，它在所有任务中看到相同的元素，并且我们要求该端每个传入元素的计算在所有任务中都是相同的。如果忽略这条规则的话，就会破坏状态的一致性保证，进而造成难以调试复现的结果。 在 processBroadcastElement() 中实现的逻辑必须在所有并行实例中具有相同的确定性行为，即不管并发多少，相同元素执行的逻辑结果是确定不变的。 由于 KeyedBroadcastProcessFunction 应用在 KeyedStream 上，相比 BroadcastProcessFunction 多了一些功能。 processElement 中的 ReadOnlyContext 允许访问 Flink 的底层计时器服务，用来注册基于事件时间和处理时间的计时器。当计时器被触发后，会调用 onTimer 方法。OnTimerContext 和 ReadOnlyContext plus 功能是一样的。 查询被触发的计时器是基于事件时间还是处理时间。 查询与计时器关联的 key。 processBroadcastElement() 方法中的 Context 包含方法 applyToKeyedState(StateDescriptor stateDescriptor, KeyedStateFunction function)。 最后回到我们最初的例子，KeyedBroadcastProcessFunction 大概就像下面这样 new KeyedBroadcastProcessFunction() { // store partial matches, i.e. first elements of the pair waiting for their second element // we keep a list as we may have many first elements waiting private final MapStateDescriptor> mapStateDesc = new MapStateDescriptor<>( \"items\", BasicTypeInfo.STRING_TYPE_INFO, new ListTypeInfo<>(Item.class)); // identical to our ruleStateDescriptor above private final MapStateDescriptor ruleStateDescriptor = new MapStateDescriptor<>( \"RulesBroadcastState\", BasicTypeInfo.STRING_TYPE_INFO, TypeInformation.of(new TypeHint() {})); @Override public void processBroadcastElement(Rule value, Context ctx, Collector out) throws Exception { ctx.getBroadcastState(ruleStateDescriptor).put(value.name, value); } @Override public void processElement(Item value, ReadOnlyContext ctx, Collector out) throws Exception { final MapState> state = getRuntimeContext().getMapState(mapStateDesc); final Shape shape = value.getShape(); for (Map.Entry entry : ctx.getBroadcastState(ruleStateDescriptor).immutableEntries()) { final String ruleName = entry.getKey(); final Rule rule = entry.getValue(); List stored = state.get(ruleName); if (stored == null) { stored = new ArrayList<>(); } if (shape == rule.second && !stored.isEmpty()) { for (Item i : stored) { out.collect(\"MATCH: \" + i + \" - \" + value); } stored.clear(); } // there is no else{} to cover if rule.first == rule.second if (shape.equals(rule.first)) { stored.add(value); } if (stored.isEmpty()) { state.remove(ruleName); } else { state.put(ruleName, stored); } } } } 1.1.4. Important Considerations 在描述了所提供的 API 之后，本节将重点介绍使用广播状态时要记住的重要事项。 没有跨任务通信：正如前面所描述的一样，只有 BroadcastProcessFunction 可以修改更新广播状态。此外，用户必须确保所有任务对每个传入元素都以相同的方式修改广播状态的内容。否则，不同的任务可能会有不同的内容，导致结果不一致。 并行 task 之间广播流的数据顺序可能不一样 每个 task 都对自己所拥有的广播状态进行 checkpoint：虽然每个 task 实例都拥有相同的广播状态，各个 task 依然各自对自己所拥有的广播状态进行 checkpoint，这样做的主要原因是为了防止 flink 程序失败从 checkpoint 恢复时全部读取相同状态文件造成的访问热点问题。这样做的问题同样显而易见，会造成状态大小的增加（依赖于任务的并行度）。Flink 保证在恢复/重新缩放时不会有重复和丢失的数据。在以相同或更小的并行度进行恢复的情况下，每个任务都会读取其检查点状态。扩大规模后，每个任务读取自己的状态，其余任务（p_new-p_old）以循环方式读取先前任务的检查点。 不支持 RocksDB 状态后端：广播状态在运行时保存在内存中，并且应该预先进行内存预估。这同样适用于所有的 operator state。 1.1.5. 思考 1. Broadcast State 的应用场景？ 规则匹配，适用于低吞吐流和高吞吐流之间的 join 操作。 2. 为什么只有 BroadcastProcessFunction 可以修改更新广播状态？一开始读了几遍，都没太看懂 By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"State & Fault Tolerance/checkpointing.html":{"url":"State & Fault Tolerance/checkpointing.html","title":"Checkpointing","keywords":"","body":"1.1.1. Checkpointing1.1.2. Checkpointing 的开启和配置1.1.3. Checkpointing 可选的配置项1.1.4. Checkpoint 存储的选择1.1.5. 迭代流中的 State Checkpoint1.1.6. 对 DAG 中已完成的节点进行 Checkpoint（BETA）1.1.7. 思考1.1.1. Checkpointing Flink 中的每个函数或者 operator 都可以是有状态的，有状态的函数可以暂时理解为在数据流接收计算过程中存储数据，比如计算五分钟之内观看直播的人次，我们就要缓存该五分钟之内进入的用户id。在类似这样的场景中，状态已然成为了程序功能组成的重要部分。 为了保证状态容错，Flink 需要缓存状态，即 Checkpoint。Checkpoint 使得 Flink 在任务失败时，可以恢复到失败时的状态和数据流的位置，以实现相同的处理语义。 Flink Checkpoint 机制需要和存储做交互（数据源存储和状态存储，不管是内存还是外部数据库），因此有几个必需点： 一个可以支持重放数据的持久化高可用的数据源，比如很多消息队列（Kafka，RabbitMQ 等）或者外部文件存储系统（HDFS，S3等） 一个可以持久化状态的存储，具有代表性的是分布式文件系统（比如 HDFS, S3, GFS, NFS, Ceph, …） 1.1.2. Checkpointing 的开启和配置 Checkpoint 默认是不开启的，需要调用 StreamExecutionEnvironment 上的 enableCheckpointing(n) 方法来启用，其中 n 代表的两次 Checkpoint 之间的 ms 间隔。 其他 checkpointing 相关的参数如下： checkpoint storage（checkpoint 的存储位置）：用来设置 checkpoint 快照的存储位置。默认情况下使用 JobManager 的堆内存。在生产环境中一般建议使用分布式文件系统。可以查看 checkpoint storage 获取更多作业或者集群方面的配置。 exactly-once vs at-least-once（精确一次处理 vs 至少一次处理）: 可以通过 enableCheckpointing 方法来设置处理语义，exactly-once 在大多数情况下性能是良好的。但是如果需要极低延时的场景下，也可以考虑使用 at-least-once。性能和准确性从来都是需要平衡的问题，需要结合自己的业务场景，比如只是简单的 ETL 处理，sink 侧可以做到幂等保证，而且需要 ms 级别的延迟保障，那么就可以使用 at-least-once。 checkpoint timeout（checkpoint 的超时时间）：在达到某个时间阈值后，checkponit 仍然没有做完，那么就终止本次 checkpoint。单位是 ms。 minimum time between checkpoints（两次 checkpoint 间隔的最小时间）：为了保证 checkpoint 已经运行了一段时间，或者说是完成，我们需要设置两次 checkpoint 之间的最小间隔。假如设置成 5000ms，则下次 checkpoint 必须在上次 checkpoint 完成 5s 后才会触发，此时与 checkpoint 间隔和运行时长无关。需要注意该参数必须小于 checkpoint interval。 tolerable checkpoint failure number（可容忍的 checkpoint 失败数）：其实就是整个作业在多少次 checkpoint 失败后可以被标记为失败。默认值是 0，即一旦有 checkpoint 失败，则整个作业就会挂掉。 number of concurrent checkpoints（可并行的 checkpoint 数量）：默认情况下，在上一个 checkpoint 未完成的情况下，不会触发下一个 checkpoint，这样做的原因主要是保证流数据处理的速度，不会被过多并发的 checkpoint 所影响。 externalized checkpoints（物化 checkpoints）：可以设置 checkpoint 写入至外部存储系统，这样在任务失败的时候就不会被清理掉，可以用来恢复数据。 unaligned checkpoints（非对齐的 checkpoints）：Flink 通过 input buffers 缓存数据和 barrier 的对齐来保证处理过程中的 exactly-once，但是在产生背压的情况下会极大拉长处理时间。非对齐的 checkpoint 可以用来解决此问题，降低处理时延，但是可能会破坏精确一次处理语义。只适用于 checkpoint 并发为 1 的情况。 checkpoints with finished tasks（对已经完成的 task 做 checkpoint）: 目前这是一个实验性的功能，当 DAG 某一部分已经完成并且处理完了所有数据（比如流批 join 的场景），后续的 checkpoint 依然会记录已完成的 task 的状态。 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // start a checkpoint every 1000 ms env.enableCheckpointing(1000); // advanced options: // set mode to exactly-once (this is the default) env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); // make sure 500 ms of progress happen between checkpoints env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500); // checkpoints have to complete within one minute, or are discarded env.getCheckpointConfig().setCheckpointTimeout(60000); // only two consecutive checkpoint failures are tolerated env.getCheckpointConfig().setTolerableCheckpointFailureNumber(2); // allow only one checkpoint to be in progress at the same time env.getCheckpointConfig().setMaxConcurrentCheckpoints(1); // enable externalized checkpoints which are retained // after job cancellation env.getCheckpointConfig().enableExternalizedCheckpoints( ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION); // enables the unaligned checkpoints env.getCheckpointConfig().enableUnalignedCheckpoints(); // sets the checkpoint storage where checkpoint snapshots will be written env.getCheckpointConfig().setCheckpointStorage(\"hdfs:///my/checkpoint/dir\") // enable checkpointing with finished tasks Configuration config = new Configuration(); config.set(ExecutionCheckpointingOptions.ENABLE_CHECKPOINTS_AFTER_TASKS_FINISH, true); env.configure(config); 1.1.3. Checkpointing 可选的配置项 可以通过 conf/flink-conf.yaml 设置更多参数或者查看对应的默认值。 参数 默认值 类型 描述 state.backend.incremental false Boolean 是否启用增量快照。对于增量快照来说，只需要存储和上次完成的 checkpoint 相比变化的数据，而不是该次全部快照，这对于状态数据非常庞大的场景是非常有用的。一旦启用该功能，flink web ui 上展示的 checkpoint 大小只会是该次变化的大小，而不是整个完整 checkpoint 的大小。并不是所有的状态后端都会支持 增量 checkpoint，目前只有 rocksDB 支持（得益于 LSM ）。 state.backend.local-recovery false Boolean 状态的本地恢复，也可以叫作任务的本地恢复。本质上是尽可能的保证失败的 task 被重新调度回原来的 slot（taskManager 粒度），进而从本地文件状态恢复，不需要在跨网络分配传输。默认情况下，本地恢复被禁用。当前本地恢复只支持 KV 类型的状态后端，基于内存的状态存储不支持本地恢复。 state.checkpoint-storage (none) string state.checkpoints.dir (none) string 用于存储状态数据及其元数据的文件目录，该目录必须允许所有节点可以访问到（即所有的 TaskManager 和 JobManager） state.checkpoints.num-retained 1 integer 允许同时保留的已完成的 checkpoint 的最大个数 state.savepoints.dir (none) string savepoints 的默认存储目录 state.storage.fs.memory-threshold 20kb MemorySize 状态数据文件的最小大小，小于这个大小的状态文件将被合并存储在 checkpoint 元数据文件中。该配置项最大可设置为 1M。 state.storage.fs.write-buffer-size 4096 Integer checkpoint 数据写入文件系统时的写入缓冲区大小。 taskmanager.state.local.root-dirs (none) String 用于本地恢复的状态数据存储的根目录，只支持 keyedStateBackend。MemoryStateBackend 不支持状态本地恢复，可以忽略此选项 1.1.4. Checkpoint 存储的选择 默认情况下，checkpoint 存储在 JobManager 的内存当中。另外对于大状态的存储，flink 也提供了不同的状态后端，可以通过 StreamExecutionEnvironment.getCheckpointConfig().setCheckpointStorage(…) 来进行配置。建议在生产环境中使用分布式文件存储系统作为状态后端。 1.1.5. 迭代流中的 State Checkpoint Flink 目前只支持非迭代数据流的状态处理。在迭代流中启用 checkpoint 会抛出异常。如果必须使用的话，可以强制开启，通过 env.enableCheckpointing(interval, CheckpointingMode.EXACTLY_ONCE, force = true) 进行配置。 1.1.6. 对 DAG 中已完成的节点进行 Checkpoint（BETA） Flink 自 1.14 开始，允许对 DAG 中已完成的节点（也可以说是 task）进行 checkpoint ，一般涉及到有界数据源。需要通过如下代码开启此功能 Configuration config = new Configuration(); config.set(ExecutionCheckpointingOptions.ENABLE_CHECKPOINTS_AFTER_TASKS_FINISH, true); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(config); 为了支持部分任务结束后的 Checkpoint 操作，我们调整了 任务的生命周期 并且引入了 StreamOperator#finish 方法。 在这一方法中，用户需要写出所有缓冲区中的数据。在 finish 方法调用后的 checkpoint 中，这一任务一定不能再有缓冲区中的数据，因为在 finish() 后没有办法来输出这些数据。 在大部分情况下，finish() 后这一任务的状态为空，唯一的例外是如果其中某些算子中包含外部系统事务的句柄（例如为了实现恰好一次语义）， 在这种情况下，在 finish() 后进行的 checkpoint 操作应该保留这些句柄，并且在结束 checkpoint（即任务退出前所等待的 checkpoint）时提交。 一个可以参考的例子是满足恰好一次语义的 sink 接口与 TwoPhaseCommitSinkFunction。 对 operator state 的影响 在部分 Task 结束后的 checkpoint 中，Flink 对 UnionListState 进行了特殊的处理。 UnionListState 一般用于实现对外部系统读取位置的一个全局视图（例如，用于记录所有 Kafka 分区的读取偏移）。 如果我们在算子的某个并发调用 close() 方法后丢弃它的状态，我们就会丢失它所分配的分区的偏移量信息。 为了解决这一问题，对于使用 UnionListState 的算子我们只允许在它的并发都在运行或都已结束的时候才能进行 checkpoint 操作。 ListState 一般不会用于类似的场景，但是用户仍然需要注意在调用 close() 方法后进行的 checkpoint 会丢弃算子的状态并且 这些状态在算子重启后不可用。 任何支持并发修改操作的算子也可以支持部分并发实例结束后的恢复操作。从这种类型的快照中恢复等价于将算子的并发改为正在运行的并发实例数。 1.1.7. 思考 1. Flink 状态存储的整个过程是什么样子的，在 stateful_stream_processing 中有涉及到，但是缺失了很多细节，需要了解下。 2. 对已经完成的 task 做 checkpoint 有什么用？ By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"State & Fault Tolerance/queryable_state.html":{"url":"State & Fault Tolerance/queryable_state.html","title":"状态查询","keywords":"","body":"1.1.1. Queryable State1.1.2. 架构设计1.1.3. 启动 Queryable State1.1.4. 如何使状态可查询1.1.5. 查询状态1.1.6. 示例1.1.7. 配置1.1.8. 局限1.1.9. 思考1.1.1. Queryable State 查询状态数据的 API 正处于不断发展更新中，对于暴露的接口不提供稳定性保障。在后续版本中，这些 API 可能会有较大的改变 简而言之，该功能暴露了 Flink 对 key state 的管理功能，用户可以在外部查询 Flink 程序运行过程中产生的状态。对于某些场景，状态可查询消除了与外部存储系统间的交互，这在某些情况下，往往是程序性能的瓶颈。除此之外，该功能对于程序调试也是非常有用的。 当查询状态对象时，该对象是并发访问的，无须同步和复制。这是一个设计方面的考虑，可以避免增大整个作业的延迟。由于使用 JVM heap 的状态后端，例如 HashMapStateBackend，不知道副本操作。当检索值而不是直接引用存储的值时，读-修改-写模式是不安全的，并且可能导致可查询状态服务器由于并发修改而失败。EmbeddedRocksDBStateBackend 可以避免这些问题。 1.1.2. 架构设计 在开始展示如何查询状态时，简要描述组成它的实体很有用。可查询状态的功能主要由以下三个部分组成 QueryableStateClient，（可能）在 Flink 集群外部运行并提交用户状态查询。 QueryableStateClientProxy，在每一个 taskManager 上运行，负责接收客户端提交的查询，获取对应 taskManager 上的状态并返回给客户端。 QueryableStateServer，在每一个 taskManager 上运行，对上面存储的状态进行管理服务。 客户端可以连接到任意其中一个代理，并发送请求查询某些 key （比如 k）关联的状态。正如 working_with_state 章节所说的一样，key state 是通过 key group 进行组织并分配在各个 taskManager 上的。为了获取 k 所在的 key group，代理会访问 jobManager，基于 jobManager 的响应。代理会查询对应 taskManager 上的 QueryableStateServer 获取 k 的状态，最后返回响应给客户端。 1.1.3. 启动 Queryable State 在 Flink 集群中启动 Queryable State 功能，需要如下步骤： 复制 Flink opt/ 目录下的 flink-queryable-state-runtime_2.11-1.14.3.jar 到 lib/。 设置 queryable-state.enable 属性为 true。 如果 Flink 集群 taskManager 日志中存在 \"Started the Queryable State Proxy Server @ ...\" 日志，则说明启动成功。 1.1.4. 如何使状态可查询 Flink 集群中已经启动了该功能，那么该如何使用呢？为了使状态对外可见，需要通过以下方式来声明： QueryableStateStream，接收上游传进来的状态值，生成可查询状态流，供后续查询。 stateDescriptor.setQueryable(String queryableStateName)，这使得状态描述符表示的 Keyed State 可查询。 下面的章节都是介绍如何使用这两种方法。 Queryable State Stream 在 KeyedStream 上调用 .asQueryableState(stateName, stateDescriptor) 方法可以生成 QueryableStateStream 供状态查询使用，根据不同类型的状态，需要使用不同的 asQueryableState 方法。 // ValueState QueryableStateStream asQueryableState( String queryableStateName, ValueStateDescriptor stateDescriptor) // Shortcut for explicit ValueStateDescriptor variant QueryableStateStream asQueryableState(String queryableStateName) // ReducingState QueryableStateStream asQueryableState( String queryableStateName, ReducingStateDescriptor stateDescriptor) 这里没有支持可查询的 ListState，因为该类型的状态一直在增长，list 中的元素得不到清理，会造成极大的内存资源消耗。 QueryableStateStream 可以被看做一个 sink，并且不能再次被转换。在内部，QueryableStateStream 被转换为一个 operator，该 operator 使用所有传入记录来更新可查询状态实例。更新逻辑取决于调用 asQueryableState() 时传入的 stateDescriptor 类型。比如像下面这样的程序中，状态就可以通过 ValueState.update(value) 来进行更新。 stream.keyBy(value -> value.f0).asQueryableState(\"query-name\") 这类似于 scala API 中的 flatMapWithState 方法。 Managed Keyed State 可以通过 StateDescriptor.setQueryable(String queryableStateName) 使 StateDescriptor 可查询，进而使该 operator 的 keyed state 可查询，如下例所示： ValueStateDescriptor> descriptor = new ValueStateDescriptor<>( \"average\", // the state name TypeInformation.of(new TypeHint>() {})); // type information descriptor.setQueryable(\"query-name\"); // queryable state name queryableStateName 参数可以任意选择，仅用于查询。不必与状态名称相同。 这种方式对使用哪种状态类型没有限制，可以应用于 ValueState、 ReduceState、ListState、MapState、和 AggregatingState。 1.1.5. 查询状态 到目前为止，Flink 集群已经可以提供状态查询服务，状态也已经变的可查询，接下来就可以了解如何查询状态。 为此，我们需要使用 QueryableStateClient 类，需要在 pom.xml 中引入此依赖。 org.apache.flink flink-core 1.14.3 org.apache.flink flink-queryable-state-client-java 1.14.3 QueryableStateClient 会提交查询请求到内部的 QueryableStateClientProxy ，用于查询状态并返回最终结果。唯一要 注意的点是我们需要给客户端指定 taskManager 的 hostname （上面已经提到过每个 taskManager 都会启动运行对应的代理服务）和代理监听的端口。关于如何配置该代理服务及其端口可以查看 Configuration Section QueryableStateClient client = new QueryableStateClient(tmHostname, proxyPort); 客户端就绪后，比如查询 k 关联的的类型为 v 的状态，可以调用该方法： CompletableFuture getKvState( JobID jobId, String queryableStateName, K key, TypeInformation keyTypeInfo, StateDescriptor stateDescriptor) 最终返回的 CompletableFuture 包含要查询的状态值。传入的 jobId 参数用来标识该状态属于哪个作业。参数 key 是待查询的 key 值，keyTypeInfo 用来确定 key 的类型以及 Flink 该如何序列化/反序列化该 key。stateDescriptor 中包含状态的类型（Value，Reduce 等等）以及该如何序列化/反序列化的信息。 细心的读者会注意到，返回的 CompletableFuture 是一个类型为 S 的值：即包含实际状态值的 State Object。可以是 Flink 支持的任意状态类型：ValueState, ReduceState, ListState, MapState, and AggregatingState。 Note: 返回的状态对象中包含的状态不允许被修改。我们可以使用该对象获取真正的状态值，例如 ValueState.get()，或者返回包含 状态对的可迭代对象，例如 mapState.entries()。我们只是不能修改这些状态，举个例子，在返回的 list state 对象上调用 add() 方法会抛出 UnsupportedOperationException。 Note: 状态客户端是异步的，且可以被多个线程共享。在想要关闭的时候必须使用 QueryableStateClient.shutdown() 方法来释放资源。 1.1.6. 示例 下面是一个是状态可查询的例子 public class CountWindowAverage extends RichFlatMapFunction, Tuple2> { private transient ValueState> sum; // a tuple containing the count and the sum @Override public void flatMap(Tuple2 input, Collector> out) throws Exception { Tuple2 currentSum = sum.value(); currentSum.f0 += 1; currentSum.f1 += input.f1; sum.update(currentSum); if (currentSum.f0 >= 2) { out.collect(new Tuple2<>(input.f0, currentSum.f1 / currentSum.f0)); sum.clear(); } } @Override public void open(Configuration config) { ValueStateDescriptor> descriptor = new ValueStateDescriptor<>( \"average\", // the state name TypeInformation.of(new TypeHint>() {})); // type information descriptor.setQueryable(\"query-name\"); sum = getRuntimeContext().getState(descriptor); } } 当包含以上逻辑的作业启动后，就可以通过 jobId 查询 sum 关联的状态。 QueryableStateClient client = new QueryableStateClient(tmHostname, proxyPort); // the state descriptor of the state to be fetched. ValueStateDescriptor> descriptor = new ValueStateDescriptor<>( \"average\", TypeInformation.of(new TypeHint>() {})); CompletableFuture>> resultFuture = client.getKvState(jobId, \"query-name\", key, BasicTypeInfo.LONG_TYPE_INFO, descriptor); // now handle the returned value resultFuture.thenAccept(response -> { try { Tuple2 res = response.get(); } catch (Exception e) { e.printStackTrace(); } }); 1.1.7. 配置 以下是 state server 和 state client 相关的一些配置，可以通过 QueryableStateOptions 进行定义。 State Server queryable-state.server.ports：state server 的端口范围。如果多个 taskManager 运行在同一台机器上，可以用来避免端口冲突。这个参数值可以被设置为多种形式，比如一个端口号：\"9123\"，或者端口号的范围：\"50100-50200\"，或者端口列表：“50100-50200,50300-50400,51234”。默认端口是 \"9067\"。 queryable-state.server.network-threads：state server 处理查询请求的网络线程数。 queryable-state.server.query-threads：state server 处理查询请求的查询线程数。 Proxy queryable-state.proxy.ports：state proxey 的端口范围。如果多个 taskManager 运行在同一台机器上，可以用来避免端口冲突。这个参数值可以被设置为多种形式，比如一个端口号：\"9123\"，或者端口号的范围：\"50100-50200\"，或者端口列表：“50100-50200,50300-50400,51234”。默认端口是 \"9069\"。 queryable-state.proxy.network-threads：state proxey 处理查询请求的网络线程数。 queryable-state.proxy.query-threads：state proxey 处理查询请求的查询线程数。 1.1.8. 局限 可查询状态的生命周期和运行的作业是绑定在一起的，在 task 启动时注册，task 完成时销毁。在未来的版本中，期望是在某个 task 结束后仍然可以查询其绑定的状态，并通过状态副本机制加速作业的恢复。 当天 KvState 的通知（个人理解是和 server，proxey 等的交互）比较简单。未来期望建立更健壮的反馈机制。 服务器和客户端跟踪查询的统计信息功能在默认情况下是被禁用的，期望在未来可以通过 metrics 进行暴露。 1.1.9. 思考 暂无 By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"State & Fault Tolerance/state_backends.html":{"url":"State & Fault Tolerance/state_backends.html","title":"状态后端","keywords":"","body":"1.1.1. 状态后端1.1.1. 状态后端 Flink 提供给了不同的状态后端用于指定状态的存储方式和位置。 状态可以存储在 Java 的堆内或者堆外内存当中。根据所使用的状态后端，Flink 可以对应管理程序的状态，这意味着 Flink 可以进行内存管理（在内存放不下时可以溢写的磁盘），以保证内存中可以放下较大的状态。默认情况下，flink_conf.yml 文件中的配置决定了所有作业该使用何种状态后端。 当然，每个作业的配置可以覆盖掉全局的配置，就像下面这样。 如果想要了解更多可用的状态后端，它们的优点，局限性，可用的配置参数等等，可以查看 Deployment & Operations 章节。 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStateBackend(...); By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"State & Fault Tolerance/state_and_fault_tolerance.html":{"url":"State & Fault Tolerance/state_and_fault_tolerance.html","title":"状态容错","keywords":"","body":"1.1.1. Overview1.1.2. 支持的数据类型1.1.3. Flink 类型管理1.1.4. 常见问题1.1.5. Flink TypeInformation 类1.1.6. Scala API 中的类型信息1.1.7. Java API 中的类型信息1.1.8. 禁用 kyro 回调1.1.9. 使用工厂类定义 TypeInformation1.1.10. 思考1.1.1. Overview Apache Flink 以其特有的方式处理数据类型和序列化，包含自己的类型描述符、泛型类型提取和类型序列化框架。本文档描述了这些概念及其背后的基本原理。 1.1.2. 支持的数据类型 Flink 对 DataStream 中的元素类型做了一些规则限制，这样做的目的是方便框架推断类型，以确定较优的执行策略。 下面是其支持的七类数据类型 1. Java Tuples and Scala Case Classes 2. Java POJOs 3. Primitive Types 4. Regular Classes 5. Values 6. Hadoop Writables 7. Special Types Tuples and Case Classes Tuples 属于复合类型，由固定数量的不同或者相同类型的字段组成。Java 本身提供了 Tuple1 到 Tuple25 的实现类。Tuple 的元素组成可以是 Flink 的任意类型，也可以嵌套 Tuple。可以直接通过 tuple.f4 或者 tuple.getField(int position) 的方式来访问 Tuple 的元素。Tuple 索引下标是从 0 开始的，这点与 Scala 不同， Scala 是通过 ._1 进行访问。 DataStream> wordCounts = env.fromElements( new Tuple2(\"hello\", 1), new Tuple2(\"world\", 2)); wordCounts.map(new MapFunction, Integer>() { @Override public Integer map(Tuple2 value) throws Exception { return value.f1; } }); wordCounts.keyBy(value -> value.f0); POJOS 在满足以下要求的时候，Java 或者 Scala 类会被 Flink 识别为 POJO 类型。 该类必须是 public 的。 该类必须含有 public（公共的）的无参构造器。 该类的属性字段必须是 public 的，或者可以通过 getter 、setter 方法访问。举个例子，对于字段 foo 来说，其对应的 getter 和 setter 方法必须被命名为 getFoo() 和 setfoo()。 该类所拥有的属性字段类型必须有对应的序列化器。 POJO 通常用 PojoTypeInfo 表示，并用 PojoSerializer 序列化。例外情况是 POJO 实际上是 Avro 类型（Avro 特定记录）或作为 “Avro 反射类型” 生成的。在这种情况下，POJO 由 AvroTypeInfo 表示并使用 AvroSerializer 序列化。你也可以在需要的时候注册自定义的序列化器，更多信息可以查看 Serialization。 Flink 会自动分析 POJO 的字段结构，在日常工作中，POJO 会比其他类型使用的频率更高且处理速度可能会更快。 你可以通过以 flink-test-utils 中的 org.apache.flink.types.PojoTestUtils#assertSerializedAsPojo() 方法来测试某个类是否属于 POJO。如果你还想确保不会使用 Kryo 序列化 POJO 的任何字段，请改用 assertSerializedAsPojoWithoutKryo()。 下面的代码展示了一个具有两个公共字段的 POJO 类 public class WordWithCount { public String word; public int count; public WordWithCount() {} public WordWithCount(String word, int count) { this.word = word; this.count = count; } } DataStream wordCounts = env.fromElements( new WordWithCount(\"hello\", 1), new WordWithCount(\"world\", 2)); wordCounts.keyBy(value -> value.word); 原语类型 Flink 支持 Java 和 Scala 的所有原语类型，比如 Integer，String，Double。 General Class Types 这里的 General Class Types 指的是泛型？ Flink 支持绝大多数的 Java 和 Scala 类（不管是 API 提供还是自定义），除了一些包含不能序列化的字段的类，比如文件指针，io 流以及一些其它计算机本地资源。遵循 Java Beans 的类型通常会运行良好。 Flink 将所有未标识为 POJO 类型（参见上面的 POJO 要求）的类作为通用类类型进行处理。Flink 将这些数据类型视为黑盒，并且不访问它们的内容（例如，为了高效排序）。General Class Types 一般使用 kyro 进行序列化/反序列化。 Values Value 类型需要手动定义描述序列化和反序列化信息。Value 没有通过通用序列化框架，而是通过使用 read 和 write 方法实现 org.apache.flink.types.Value 接口，为序列化操作提供自定义代码。使用 Value Type 的原因往往是通用序列化带来的性能低下的问题。比如一个稀疏元素的数组，我们在知道这个数组大多数都是空值的情况下，可以对空值元素使用特殊的编码方式，而通用的序列化框架往往直接把整个数组的元素进行写入与读取。 org.apache.flink.types.CopyableValue 接口以类似的方式支持内部复制逻辑。 Flink 的基本类型自带预定义的 Value 类型（ByteValue, ShortValue, IntValue, LongValue, FloatValue, DoubleValue, StringValue, CharValue, BooleanValue）。这些 Value 类型充当基本数据类型的可变变体，它们的值可以改变、也允许复用已减少 GC 压力。 Hadoop Writables 你可以使用实现 org.apache.hadoop.Writable 接口的类型。 write() 和 readFields() 方法中定义的序列化逻辑将用于序列化。 特殊类型 你可以使用特殊类型，比如 Scala 中的 Either、Option、Try 类型。Java 本身有自己的 Either 实现，和 Scala 中的 Either 类似，其提供了两种可能的数据返回，left 或者 right。Either 常用于错误处理以及需要输出不同类型记录的场景。 类型擦除 & 类型推断 需要注意该小节仅针对于 Java。 Java 编译器在编译后丢弃了泛型类型信息，这在 Java 里被叫做类型擦除。这意味着在运行时，一个对象实例可能并不知道自己的类型信息。比如，DataStream 和 DataStream 在 JVM 看来是一样的。 Flink 程序在执行时需要类型信息，Flink Java API 试图通过各种方式重构被丢弃的类型信息，并将其显式存储在数据集和对应的算子中。 您可以通过 DataStream.getType() 检索类型，该方法返回一个 TypeInformation 实例，这是 Flink 内部表示类型的方式。 类型推断具有局限性，在某些场景下需要程序编写者的配合。例如，从集合中创建数据集的方法，StreamExecutionEnvironment.fromCollection()，你可以在其中传递描述类型的参数。但像 MapFunction 这样的通用函数也可能需要额外的类型信息。 ResultTypeQueryable 接口可以通过输入格式和函数来实现，以明确地告诉 API 它们的返回类型。调用函数的输入类型通常可以通过先前操作的结果类型来推断。 1.1.3. Flink 类型管理 Flink 会尝试获取更多的类型信息以优化分布式计算中交换、存储数据的性能。可以把其想象成一个可以推断表结构的数据库（emmn，这个比喻没太 get 到）。通过类型信息，Flink 可以做很多优化上的事情： Flink 获取到的类型信息越多，其序列化和数据分布的规划就会做的更好。这对于 Flink 本身的内存使用非常重要（在堆内/堆外内存中处理序列化数据，更好的序列化器选择也会使序列化过程更叫高效和廉价）。 在很多情况下，用户不必关心序列化框架在做什么，也不用额外注册类型信息。 通常在程序真正执行前会尝试获取类型信息，比如对 DataStream 进行调用，或者在 execute()、print()、count() 或 collect() 等方法调用之前。 1.1.4. 常见问题 以下是用户在处理 Flink 类型中最常见的几个问题： Registering subtypes：注册子类型，如果一个函数签名使用了父类型，但是在真正执行的时候使用的传参是该父类型的子类，让 Flink 获取到我们使用了这些子类型会极大的提升程序性能。为此，可以在 StreamExecutionEnvironment 上为每个子类型调用 .registerType(clazz) 方法。 Registering custom serializers：注册自定义的序列化器，Flink 在遇到无法处理的类型时会调用 kyro 序列化器。但是 kyro 并非能处理所有类型。例如，许多 Google Guava 集合类型在默认情况下无法正常工作，解决办法就是为这些类型注册额外的序列化器。可以在 StreamExecutionEnvironment 上调用 .getConfig().addDefaultKryoSerializer(clazz, serializer) 方法。许多库中都提供了额外的 Kryo 序列化器。有关使用外部序列化器的更多详细信息，请参考3rd party serializer。 Adding Type Hints：添加类型提示信息，有时，当 Flink 使用了所有技巧仍无法推断泛型类型时，用户必须传递类型提示信息。 这通常只在 Java API 中是必需的。关于类型提示的更多信息，可以参阅Type Hints Section Manually creating a TypeInformation：手动创建 TypeInformation，由于 Java 的泛型类型擦除，Flink 无法推断数据类型的某些 API 调用可能需要这样做。可以参阅 Creating a TypeInformation or TypeSerializer 以了解更多。 1.1.5. Flink TypeInformation 类 TypeInformation 类是所有类型描述的基类。它定义了类型的一些基本属性，并可以生成对应的序列化器，并且还可以生成相应类型的比较器。（需要注意，Flink 中的比较器不仅仅是定义一个顺序，它们常用来处理 key）。这个位置没太明白，Flink 的 comparators 还可以做什么呢？ 在内部，Flink 对类型进行了以下归类划分： 基本类型：所有 Java 的原语类型及其封装，加上 void、StringDate、BigDecimal、BigInteger。 原始数组和对象数组。 复合类型 Java Tuples，最多 25 个元素，不支持 null 值。 Scala 样例类（包括 Tuples），不支持 null 值。 Row：可以理解为带有可选 Schema 信息的 Tuple，没有数量限制，允许 null 值存在。 POJOs：遵循 Java Bean 规则的类。 辅助类型：（Option、Either、Lists、Maps、...） 泛型：Flink 本身不处理其序列化，而是交给 kyro 来做。 POJOs 使用非常广泛，因为其支持组合任意复杂类型。且 Flink 对 POJOs 的处理也相对高效。 POJO 类型的规则 如果满足以下条件，Flink 会将数据类型识别为 POJO 类型（并允许按字段名称进行引用）： class 是 public 的并且是单例（不包括静态内部类）。 该类有一个公共的无参构造器。 该类的所有非静态字段，包括其子类，必须是 public 的。或者有相对应的公共 getter 和 setter 方法。 需要注意的是当用户自定义的数据类型不能被识别为 POJOs 时，会被当做泛型，使用 kyro 序列化处理。 创建一个 TypeInformation 类及其对应的序列化器 因为 Java 的泛型擦除原因，所以需要将类型传递给 TypeInformation 构造器。对于非泛型，可以使用如下方法。 TypeInformation info = TypeInformation.of(String.class); 对于泛型，必须使用 TypeHint 标识泛型类型信息 TypeInformation> info = TypeInformation.of(new TypeHint>(){}); 可以在 TypeInformation 实例上调用 typeInfo.createSerializer(config) 方法创建对应类型的序列化器。其中的 config 参数是 ExecutionConfig 类型，包含有关程序注册的自定义序列化程序的信息，可以在 DataStream 上调用 getExecutionConfig() 方法。在函数内部（如 MapFunction），您可以通过将函数设为 Rich Function 并调用 getRuntimeContext().getExecutionConfig() 来获取它。 1.1.6. Scala API 中的类型信息 Scala 通过类型 manifests 和类标签对运行时的类型信息有非常详尽的了解。通常，类型和方法可以访问它们的泛型参数的类型，因此Scala 程序不会像 Java 程序那样遭受类型擦除的困扰。 此外，Scala 允许通过 Scala 宏在 Scala 编译器中运行自定义代码。that means that some Flink code gets executed whenever you compile a Scala program written against Flink’s Scala API。此处有点不太理解，对于程序的底层编译运行还是一知半解。 Scala 宏可以在编译期获取用户自定义函数的参数和返回值类型。在宏中，我们为参数或者返回值的类型创建一个 TypeInformation 并注入到后续的 operator 中。 No Implicit Value for Evidence Parameter Error 这是使用 Scala API 在创建 TypeInformation 时的一个常见错误。其原因往往是没有导入完整的 flink.api.scala 类库。 import org.apache.flink.api.scala._ 另一个常见的原因是泛型方法引起的，这个在下一小节会提到。 泛型方法 可以先思考下下面的代码： def selectFirst[T](input: DataStream[(T, _)]) : DataStream[T] = { input.map { v => v._1 } } val data : DataStream[(String, Long) = ... val result = selectFirst(data) 对于此类泛型方法，函数参数的数据类型和返回值类型对于每次调用可能都不相同，并且在方法签名中也是未知的。这个也会造成 No Implicit Value for Evidence Parameter Error。 在这种情况下，类型信息必须在调用时生成并传递给方法。 Scala 为此提供了隐式参数。以下代码告诉 Scala 将 T 的类型信息带入函数中。然后将在调用方法的位置而不是定义方法的位置生成类型信息。 def selectFirst[T : TypeInformation](input: DataStream[(T, _)]) : DataStream[T] = { input.map { v => v._1 } } 1.1.7. Java API 中的类型信息 通常情况下，Java 会擦除泛型的类型信息。Flink 试图通过 Java 本身保留的少数位信息（主要是函数签名和子类信息）通过反射重建尽可能多的类型信息。此逻辑还包含一些简单的类型推断，适用于函数的返回类型取决于其输入类型的情况： public class AppendOne implements MapFunction> { public Tuple2 map(T value) { return new Tuple2(value, 1L); } } 这里同样存在 Flink 无法补齐的类型信息，此时需要 TypeHint 辅助重建。 Type Hints in the Java API 在Flink 无法重建泛型类型信息的情况下，Java API 提供了 type hints 用来提示告诉系统该函数产生的数据集类型。 DataStream result = stream .map(new MyGenericNonInferrableFunction()) .returns(SomeType.class); returns() 方法指定生成的类型，在本例中是通过一个类。 class，无参类，且不是泛型。 returns() 方法返回 TypeHints(new TypeHint>(){})。 TypeHint 类可以捕获泛型类型信息并为运行时保留它（通过匿名子类的方式）。 Type extraction for Java 8 lambdas Java 8 lambda 的类型提取与非 lambda 的工作方式不同，因为 lambda 不与继承函数接口的实现类相关联。 目前，Flink 试图弄清楚哪个方法实现了 lambda，并使用 Java 的泛型签名来确定参数类型和返回类型。但是，并非所有编译器都为 lambda 生成这些签名。如果你观察到意外行为，请使用 returns 方法手动指定返回类型。 Serialization of POJO types PojoTypeInfo 为 POJO 中的所有字段创建序列化程器。标准类型如 int、long、String 等，由 Flink 提供的序列化程序处理。对于其他类型，会使用 Kryo 序列化。 如果 kyro 不能正常处理这些类型，可以使用 avro。 final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.getConfig().enableForceAvro(); 也可以自定义序列化器。 env.getConfig().addDefaultKryoSerializer(Class type, Class> serializerClass); 1.1.8. 禁用 kyro 回调 在某些情况下，我们可能希望明确避免使用 Kryo 作为泛型类型的备选序列化器。最常见的一种是希望通过 Flink 自己的序列化器或用户定义的自定义序列化器来确保所有类型都被有效地序列化。 env.getConfig().disableGenericTypes(); 1.1.9. 使用工厂类定义 TypeInformation TypeInformation 的工厂类允许以可插拔的形式将自定义类型注入 Flink 本身的类型系统。我们必须继承 org.apache.flink.api.common.typeinfo.TypeInfoFactory 才能返回我们的自定义类型信息。如果相应类型或使用此类型的 POJO 字段已使用 @org.apache.flink.api.common.typeinfo.TypeInfo 注释进行注释，则在类型提取阶段调用会调用工厂类。该方法在 Java 和 Scala API 中是通用的。 在类型层次结构中，会选择最近的类型工厂，当然内置的类型工厂是优先级最高的。以下示例显示如何使用 Java 中的工厂注释自定义类型 MyTuple 并为其提供自定义类型信息。 @TypeInfo(MyTupleTypeInfoFactory.class) public class MyTuple { public T0 myfield0; public T1 myfield1; } public class MyTupleTypeInfoFactory extends TypeInfoFactory { @Override public TypeInformation createTypeInfo(Type t, Map> genericParameters) { return new MyTupleTypeInfo(genericParameters.get(\"T0\"), genericParameters.get(\"T1\")); } } 除了注解类型本身，还可以在 Flink POJO 中使用，如下所示： public class MyPojo { public int id; @TypeInfo(MyTupleTypeInfoFactory.class) public MyTuple tuple; } createTypeInfo(Type, Map>) 方法为工厂的目标类型创建类型信息。其参数提供有关类型本身以及类型的泛型参数（如果可用）的额外信息。 1.1.10. 思考 1. POJO 的序列化，PojoTypeInfo 是啥，和 kyro、avro 又是啥关系？ 2. Java 或者 Scala 的原语类型是指啥？ 一般分为原语类型和类类型，原语类型比如 Java 的 int、boolean、long 等，既不是对象也不是类，也没有实现什么方法。而像 String，Integer 类型则是类类型，可以实例化，有丰富的方法可以调用。 3. Java 的类型擦除？ 可以查看这篇文档 Java泛型类型擦除以及类型擦除带来的问题 4. Scala 宏是什么？ 神奇的Scala Macro之旅 By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"State & Fault Tolerance/state_schema_evolution.html":{"url":"State & Fault Tolerance/state_schema_evolution.html","title":"状态迁移与演化","keywords":"","body":"1.1.1. Overview1.1.2. 状态 schema 的演化1.1.3. 思考1.1.1. Overview Flink 常常用于实时流处理，其特点是长时间运行。和其它长时间运行的服务类似，我们也需要更新程序以应对不断变化的需求。对存在于应用程序中的 schema 信息也是如此，其会随着应用程序的更新而演化发展。 本章节主要讲解如何更新升级状态的 schema 信息。对于不同的状态结构和类型，其实现会有所差异。需要注意的是仅当您使用由 Flink 自己的类型序列化框架生成的状态序列化程序时，此页面上的信息才相关。也就是说，在声明状态时，提供的状态描述符未配置为使用特定的 TypeSerializer 或 TypeInformation，在这种情况下，Flink 会推断有关状态类型的信息： ListStateDescriptor descriptor = new ListStateDescriptor<>( \"state-name\", MyPojoType.class); checkpointedState = getRuntimeContext().getListState(descriptor); 本质上，状态 schema 是否可以自动推断取决于用于读取/写入持久化状态的程序。简单来说，一个状态的 schema 只有在使用正确的序列化时才可以自动推断演化。这由 Flink 的类型序列化框架生成的序列化程序透明地处理。 如果您打算为您的状态类型实现自定义 TypeSerializer，并且想了解如何实现序列化器以支持状态模式演化，可以查看Custom State Serialization 1.1.2. 状态 schema 的演化 总结一句就是以旧模式读取旧数据，以新模式继续处理新的数据。 1.1.3. 思考 1. 状态的演化是啥含义？状态的 schema 为啥会发生改变？ By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"State & Fault Tolerance/custom_state_serialization.html":{"url":"State & Fault Tolerance/custom_state_serialization.html","title":"自定义状态序列化器","keywords":"","body":" By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"DataSet API/":{"url":"DataSet API/","title":"DataSet API","keywords":"","body":" 导航信息 By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"Table API/":{"url":"Table API/","title":"Table API","keywords":"","body":"1.1.1. Table API1.1.1. Table API By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "},"Data Types & Serialization/":{"url":"Data Types & Serialization/","title":"Data Types & Serialization","keywords":"","body":" By Flyraty，使用知识共享 署名-相同方式共享 4.0协议发布            updated 2023-01-10 14:39:27 "}}